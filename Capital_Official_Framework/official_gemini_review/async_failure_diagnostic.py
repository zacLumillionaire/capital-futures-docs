#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç•°æ­¥å¤±æ•—è¨ºæ–·å·¥å…· - æ·±åº¦åˆ†æç·©å­˜å‘½ä¸­å¢åŠ ä½†å¤±æ•—ä»»å‹™ä¹Ÿå¢åŠ çš„å•é¡Œ
"""

import time
import threading
import sqlite3
import os
import traceback
from datetime import datetime, timedelta

def run_diagnostic_on_simple_integrated(main_app):
    """åœ¨simple_integratedå¯¦ä¾‹ä¸Šé‹è¡Œå®Œæ•´è¨ºæ–·"""
    print("\nğŸ”¬ ç•°æ­¥å¤±æ•—æ·±åº¦è¨ºæ–·é–‹å§‹...")
    print("=" * 70)
    
    diagnostic_results = {
        'timestamp': datetime.now().isoformat(),
        'database_analysis': {},
        'memory_analysis': {},
        'thread_analysis': {},
        'performance_analysis': {},
        'error_analysis': {},
        'recommendations': []
    }
    
    # 1. æ•¸æ“šåº«æ·±åº¦åˆ†æ
    print("ğŸ“‹ æ­¥é©Ÿ1: æ•¸æ“šåº«æ·±åº¦åˆ†æ...")
    diagnostic_results['database_analysis'] = analyze_database_issues(main_app)
    
    # 2. å…§å­˜ä½¿ç”¨åˆ†æ
    print("\nğŸ“‹ æ­¥é©Ÿ2: å…§å­˜ä½¿ç”¨åˆ†æ...")
    diagnostic_results['memory_analysis'] = analyze_memory_usage(main_app)
    
    # 3. ç·šç¨‹ç‹€æ…‹åˆ†æ
    print("\nğŸ“‹ æ­¥é©Ÿ3: ç·šç¨‹ç‹€æ…‹åˆ†æ...")
    diagnostic_results['thread_analysis'] = analyze_thread_status(main_app)
    
    # 4. æ€§èƒ½ç“¶é ¸åˆ†æ
    print("\nğŸ“‹ æ­¥é©Ÿ4: æ€§èƒ½ç“¶é ¸åˆ†æ...")
    diagnostic_results['performance_analysis'] = analyze_performance_bottlenecks(main_app)
    
    # 5. éŒ¯èª¤æ¨¡å¼åˆ†æ
    print("\nğŸ“‹ æ­¥é©Ÿ5: éŒ¯èª¤æ¨¡å¼åˆ†æ...")
    diagnostic_results['error_analysis'] = analyze_error_patterns(main_app)
    
    # 6. ç”Ÿæˆå»ºè­°
    print("\nğŸ“‹ æ­¥é©Ÿ6: ç”Ÿæˆä¿®å¾©å»ºè­°...")
    diagnostic_results['recommendations'] = generate_recommendations(diagnostic_results)
    
    # 7. è¼¸å‡ºè¨ºæ–·å ±å‘Š
    print_diagnostic_report(diagnostic_results)
    
    return diagnostic_results

def analyze_database_issues(main_app):
    """åˆ†ææ•¸æ“šåº«ç›¸é—œå•é¡Œ"""
    analysis = {
        'connection_pool': 'unknown',
        'lock_conflicts': 0,
        'transaction_time': 0,
        'table_sizes': {},
        'index_efficiency': 'unknown'
    }
    
    try:
        db_path = "multi_group_strategy.db"
        if not os.path.exists(db_path):
            analysis['connection_pool'] = 'no_database'
            return analysis
        
        conn = sqlite3.connect(db_path, timeout=10.0)
        cursor = conn.cursor()
        
        # æª¢æŸ¥è¡¨å¤§å°
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = cursor.fetchall()
        
        for (table_name,) in tables:
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                count = cursor.fetchone()[0]
                analysis['table_sizes'][table_name] = count
            except:
                analysis['table_sizes'][table_name] = 'error'
        
        # æ¸¬è©¦äº‹å‹™æ™‚é–“
        start_time = time.time()
        cursor.execute("BEGIN TRANSACTION")
        cursor.execute("CREATE TABLE IF NOT EXISTS perf_test (id INTEGER)")
        cursor.execute("INSERT INTO perf_test VALUES (1)")
        cursor.execute("DELETE FROM perf_test WHERE id = 1")
        cursor.execute("COMMIT")
        analysis['transaction_time'] = time.time() - start_time
        
        # æª¢æŸ¥é–å®šæƒ…æ³
        cursor.execute("PRAGMA database_list")
        analysis['connection_pool'] = 'healthy'
        
        conn.close()
        
        print(f"   ğŸ“Š æ•¸æ“šåº«è¡¨æ•¸é‡: {len(analysis['table_sizes'])}")
        print(f"   â±ï¸ äº‹å‹™æ™‚é–“: {analysis['transaction_time']:.3f}ç§’")
        
        if analysis['transaction_time'] > 1.0:
            print(f"   âš ï¸ è­¦å‘Š: äº‹å‹™æ™‚é–“éé•·")
        
    except Exception as e:
        print(f"   âŒ æ•¸æ“šåº«åˆ†æå¤±æ•—: {e}")
        analysis['connection_pool'] = 'error'
    
    return analysis

def analyze_memory_usage(main_app):
    """åˆ†æå…§å­˜ä½¿ç”¨æƒ…æ³"""
    analysis = {
        'cache_sizes': {},
        'memory_leaks': [],
        'cache_hit_rates': {},
        'memory_efficiency': 'unknown'
    }
    
    try:
        # æª¢æŸ¥OptimizedRiskManagerç·©å­˜
        if hasattr(main_app, 'optimized_risk_manager') and main_app.optimized_risk_manager:
            manager = main_app.optimized_risk_manager
            
            caches = ['position_cache', 'stop_loss_cache', 'activation_cache', 'trailing_cache']
            for cache_name in caches:
                if hasattr(manager, cache_name):
                    cache = getattr(manager, cache_name)
                    if isinstance(cache, dict):
                        analysis['cache_sizes'][f'risk_manager_{cache_name}'] = len(cache)
        
        # æª¢æŸ¥AsyncUpdaterç·©å­˜
        if hasattr(main_app, 'async_updater') and main_app.async_updater:
            updater = main_app.async_updater
            
            if hasattr(updater, 'memory_cache'):
                for cache_type, cache_data in updater.memory_cache.items():
                    if isinstance(cache_data, dict):
                        analysis['cache_sizes'][f'async_{cache_type}'] = len(cache_data)
            
            # æª¢æŸ¥ç·©å­˜å‘½ä¸­ç‡
            if hasattr(updater, 'stats'):
                stats = updater.stats
                total_tasks = stats.get('total_tasks', 0)
                cache_hits = stats.get('cache_hits', 0)
                
                if total_tasks > 0:
                    hit_rate = (cache_hits / total_tasks) * 100
                    analysis['cache_hit_rates']['async_updater'] = hit_rate
        
        # æª¢æŸ¥SimplifiedTrackerç‹€æ…‹
        if hasattr(main_app, 'multi_group_position_manager') and main_app.multi_group_position_manager:
            if hasattr(main_app.multi_group_position_manager, 'simplified_tracker'):
                tracker = main_app.multi_group_position_manager.simplified_tracker
                
                if hasattr(tracker, 'strategy_groups'):
                    analysis['cache_sizes']['strategy_groups'] = len(tracker.strategy_groups)
                
                if hasattr(tracker, 'exit_groups'):
                    analysis['cache_sizes']['exit_groups'] = len(tracker.exit_groups)
        
        # æª¢æŸ¥å…§å­˜æ´©æ¼è·¡è±¡
        total_cache_items = sum(analysis['cache_sizes'].values())
        if total_cache_items > 1000:
            analysis['memory_leaks'].append(f"ç¸½ç·©å­˜é …ç›®éå¤š: {total_cache_items}")
        
        print(f"   ğŸ“Š ç·©å­˜çµ±è¨ˆ:")
        for cache_name, size in analysis['cache_sizes'].items():
            print(f"      - {cache_name}: {size} é …")
        
        if analysis['cache_hit_rates']:
            print(f"   ğŸ“Š ç·©å­˜å‘½ä¸­ç‡:")
            for component, rate in analysis['cache_hit_rates'].items():
                print(f"      - {component}: {rate:.1f}%")
        
    except Exception as e:
        print(f"   âŒ å…§å­˜åˆ†æå¤±æ•—: {e}")
    
    return analysis

def analyze_thread_status(main_app):
    """åˆ†æç·šç¨‹ç‹€æ…‹"""
    analysis = {
        'active_threads': 0,
        'thread_details': [],
        'deadlock_risk': 'low',
        'thread_efficiency': 'unknown'
    }
    
    try:
        # ç²å–æ‰€æœ‰æ´»èºç·šç¨‹
        all_threads = threading.enumerate()
        analysis['active_threads'] = len(all_threads)
        
        for thread in all_threads:
            thread_info = {
                'name': thread.name,
                'alive': thread.is_alive(),
                'daemon': thread.daemon,
                'ident': thread.ident
            }
            analysis['thread_details'].append(thread_info)
        
        # æª¢æŸ¥AsyncUpdaterå·¥ä½œç·šç¨‹
        if hasattr(main_app, 'async_updater') and main_app.async_updater:
            updater = main_app.async_updater
            if hasattr(updater, 'worker_thread'):
                worker = updater.worker_thread
                if worker and worker.is_alive():
                    print(f"   âœ… AsyncUpdaterå·¥ä½œç·šç¨‹æ­£å¸¸")
                else:
                    print(f"   âŒ AsyncUpdaterå·¥ä½œç·šç¨‹ç•°å¸¸")
                    analysis['deadlock_risk'] = 'high'
        
        print(f"   ğŸ“Š æ´»èºç·šç¨‹æ•¸: {analysis['active_threads']}")
        
        if analysis['active_threads'] > 20:
            print(f"   âš ï¸ è­¦å‘Š: ç·šç¨‹æ•¸é‡è¼ƒå¤š")
            analysis['deadlock_risk'] = 'medium'
        
    except Exception as e:
        print(f"   âŒ ç·šç¨‹åˆ†æå¤±æ•—: {e}")
    
    return analysis

def analyze_performance_bottlenecks(main_app):
    """åˆ†ææ€§èƒ½ç“¶é ¸"""
    analysis = {
        'queue_latency': 0,
        'database_latency': 0,
        'cache_efficiency': 'unknown',
        'bottleneck_sources': []
    }
    
    try:
        # æ¸¬è©¦æ•¸æ“šåº«å»¶é²
        start_time = time.time()
        try:
            conn = sqlite3.connect("multi_group_strategy.db", timeout=5.0)
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            cursor.fetchone()
            conn.close()
            analysis['database_latency'] = time.time() - start_time
        except:
            analysis['database_latency'] = -1
        
        # æª¢æŸ¥éšŠåˆ—å»¶é²
        if hasattr(main_app, 'async_updater') and main_app.async_updater:
            updater = main_app.async_updater
            if hasattr(updater, 'task_queue'):
                queue_size = updater.task_queue.qsize()
                if queue_size > 50:
                    analysis['bottleneck_sources'].append(f"ä»»å‹™éšŠåˆ—éå¤§: {queue_size}")
        
        print(f"   â±ï¸ æ•¸æ“šåº«å»¶é²: {analysis['database_latency']:.3f}ç§’")
        
        if analysis['database_latency'] > 0.1:
            print(f"   âš ï¸ è­¦å‘Š: æ•¸æ“šåº«å»¶é²è¼ƒé«˜")
            analysis['bottleneck_sources'].append("æ•¸æ“šåº«å»¶é²éé«˜")
        
    except Exception as e:
        print(f"   âŒ æ€§èƒ½åˆ†æå¤±æ•—: {e}")
    
    return analysis

def analyze_error_patterns(main_app):
    """åˆ†æéŒ¯èª¤æ¨¡å¼"""
    analysis = {
        'recent_errors': [],
        'error_frequency': {},
        'critical_errors': [],
        'error_trends': 'unknown'
    }
    
    try:
        # æª¢æŸ¥AsyncUpdaterçµ±è¨ˆ
        if hasattr(main_app, 'async_updater') and main_app.async_updater:
            updater = main_app.async_updater
            if hasattr(updater, 'stats'):
                stats = updater.stats
                failed_tasks = stats.get('failed_tasks', 0)
                total_tasks = stats.get('total_tasks', 0)
                
                if total_tasks > 0:
                    failure_rate = (failed_tasks / total_tasks) * 100
                    analysis['error_frequency']['async_updater'] = failure_rate
                    
                    if failure_rate > 10:
                        analysis['critical_errors'].append(f"AsyncUpdaterå¤±æ•—ç‡éé«˜: {failure_rate:.1f}%")
        
        print(f"   ğŸ“Š éŒ¯èª¤çµ±è¨ˆ:")
        for component, rate in analysis['error_frequency'].items():
            print(f"      - {component}: {rate:.1f}% å¤±æ•—ç‡")
        
        if analysis['critical_errors']:
            print(f"   ğŸš¨ é—œéµéŒ¯èª¤:")
            for error in analysis['critical_errors']:
                print(f"      - {error}")
        
    except Exception as e:
        print(f"   âŒ éŒ¯èª¤åˆ†æå¤±æ•—: {e}")
    
    return analysis

def generate_recommendations(diagnostic_results):
    """ç”Ÿæˆä¿®å¾©å»ºè­°"""
    recommendations = []
    
    # åŸºæ–¼æ•¸æ“šåº«åˆ†æçš„å»ºè­°
    db_analysis = diagnostic_results.get('database_analysis', {})
    if db_analysis.get('transaction_time', 0) > 1.0:
        recommendations.append({
            'priority': 'high',
            'category': 'database',
            'issue': 'æ•¸æ“šåº«äº‹å‹™æ™‚é–“éé•·',
            'solution': 'èª¿æ•´æ•¸æ“šåº«è¶…æ™‚è¨­ç½®ï¼Œå„ªåŒ–æŸ¥è©¢èªå¥'
        })
    
    # åŸºæ–¼å…§å­˜åˆ†æçš„å»ºè­°
    memory_analysis = diagnostic_results.get('memory_analysis', {})
    if memory_analysis.get('memory_leaks'):
        recommendations.append({
            'priority': 'medium',
            'category': 'memory',
            'issue': 'å¯èƒ½çš„å…§å­˜æ´©æ¼',
            'solution': 'å®šæœŸæ¸…ç†ç·©å­˜ï¼Œå¯¦æ–½å…§å­˜ç›£æ§'
        })
    
    # åŸºæ–¼ç·šç¨‹åˆ†æçš„å»ºè­°
    thread_analysis = diagnostic_results.get('thread_analysis', {})
    if thread_analysis.get('deadlock_risk') == 'high':
        recommendations.append({
            'priority': 'high',
            'category': 'threading',
            'issue': 'ç·šç¨‹æ­»é–é¢¨éšª',
            'solution': 'é‡å•Ÿç•°æ­¥æ›´æ–°å™¨ï¼Œæª¢æŸ¥ç·šç¨‹åŒæ­¥é‚è¼¯'
        })
    
    # åŸºæ–¼éŒ¯èª¤åˆ†æçš„å»ºè­°
    error_analysis = diagnostic_results.get('error_analysis', {})
    if error_analysis.get('critical_errors'):
        recommendations.append({
            'priority': 'critical',
            'category': 'errors',
            'issue': 'é—œéµéŒ¯èª¤éœ€è¦ç«‹å³è™•ç†',
            'solution': 'å•Ÿç”¨è©³ç´°æ—¥èªŒï¼Œåˆ†æå…·é«”éŒ¯èª¤åŸå› '
        })
    
    return recommendations

def print_diagnostic_report(diagnostic_results):
    """è¼¸å‡ºè¨ºæ–·å ±å‘Š"""
    print("\nğŸ“Š è¨ºæ–·å ±å‘Šç¸½çµ")
    print("=" * 70)
    
    recommendations = diagnostic_results.get('recommendations', [])
    
    if not recommendations:
        print("âœ… æœªç™¼ç¾é‡å¤§å•é¡Œ")
        return
    
    # æŒ‰å„ªå…ˆç´šåˆ†çµ„
    critical = [r for r in recommendations if r['priority'] == 'critical']
    high = [r for r in recommendations if r['priority'] == 'high']
    medium = [r for r in recommendations if r['priority'] == 'medium']
    
    if critical:
        print("\nğŸš¨ é—œéµå•é¡Œ (ç«‹å³è™•ç†):")
        for rec in critical:
            print(f"   - {rec['issue']}")
            print(f"     è§£æ±ºæ–¹æ¡ˆ: {rec['solution']}")
    
    if high:
        print("\nâš ï¸ é«˜å„ªå…ˆç´šå•é¡Œ:")
        for rec in high:
            print(f"   - {rec['issue']}")
            print(f"     è§£æ±ºæ–¹æ¡ˆ: {rec['solution']}")
    
    if medium:
        print("\nğŸ’¡ ä¸­å„ªå…ˆç´šå•é¡Œ:")
        for rec in medium:
            print(f"   - {rec['issue']}")
            print(f"     è§£æ±ºæ–¹æ¡ˆ: {rec['solution']}")
    
    print(f"\nğŸ“‹ ç¸½è¨ˆç™¼ç¾ {len(recommendations)} å€‹å•é¡Œéœ€è¦è™•ç†")

if __name__ == "__main__":
    print("ç•°æ­¥å¤±æ•—è¨ºæ–·å·¥å…·")
    print("è«‹åœ¨ simple_integrated.py ä¸­èª¿ç”¨æ­¤æ¨¡çµ„çš„å‡½æ•¸")
