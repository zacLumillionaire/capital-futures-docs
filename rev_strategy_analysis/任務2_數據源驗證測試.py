#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»»å‹™ 2ï¼šæ•¸æ“šæºèˆ‡è¼¸å…¥é©—è­‰æ¸¬è©¦è…³æœ¬
é©—è­‰ mdd_gui.py å’Œ rev_web_trading_gui.py ä½¿ç”¨çš„æ•¸æ“šæºæ˜¯å¦å®Œå…¨ä¸€è‡´
"""

import sys
import os
import logging
from datetime import time, date
from pathlib import Path
import hashlib
import json

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
parent_dir = Path(__file__).parent
sys.path.insert(0, str(parent_dir))

# å°å…¥æ•¸æ“šæºæ¨¡çµ„
try:
    import sqlite_connection
    from app_setup import init_all_db_pools
    import shared
except ImportError as e:
    logger.error(f"ç„¡æ³•å°å…¥å¿…è¦æ¨¡çµ„: {e}")
    sys.exit(1)

class DataSourceValidator:
    """æ•¸æ“šæºé©—è­‰å™¨"""
    
    def __init__(self):
        self.use_sqlite = True  # èˆ‡å…©å€‹ç³»çµ±ä¿æŒä¸€è‡´
        self.test_date = "2024-11-15"  # æ¸¬è©¦æ—¥æœŸ
        self.results = {}
        
    def init_data_source(self):
        """åˆå§‹åŒ–æ•¸æ“šæº"""
        if self.use_sqlite:
            try:
                sqlite_connection.init_sqlite_connection()
                logger.info("âœ… SQLiteé€£æ¥åˆå§‹åŒ–æˆåŠŸ")
                return True
            except Exception as e:
                logger.error(f"âŒ SQLiteé€£æ¥åˆå§‹åŒ–å¤±æ•—: {e}")
                return False
        else:
            try:
                init_all_db_pools()
                logger.info("âœ… PostgreSQLé€£ç·šæ± åˆå§‹åŒ–æˆåŠŸ")
                return True
            except Exception as e:
                logger.error(f"âŒ PostgreSQLé€£ç·šæ± åˆå§‹åŒ–å¤±æ•—: {e}")
                return False
    
    def get_data_source_info(self, source_name):
        """ç²å–æ•¸æ“šæºä¿¡æ¯"""
        logger.info(f"ğŸ” æª¢æŸ¥æ•¸æ“šæº: {source_name}")
        
        try:
            if self.use_sqlite:
                context_manager = sqlite_connection.get_conn_cur_from_sqlite_with_adapter(as_dict=True)
            else:
                context_manager = shared.get_conn_cur_from_pool_b(as_dict=True)
            
            with context_manager as (conn, cur):
                # 1. æª¢æŸ¥æ•¸æ“šåº«æ–‡ä»¶/é€£æ¥ä¿¡æ¯
                if self.use_sqlite:
                    db_path = sqlite_connection._sqlite_connection.db_path
                    db_size = db_path.stat().st_size if db_path.exists() else 0
                    db_mtime = db_path.stat().st_mtime if db_path.exists() else 0
                    
                    source_info = {
                        'database_type': 'SQLite',
                        'database_path': str(db_path),
                        'database_size': db_size,
                        'database_mtime': db_mtime,
                        'database_exists': db_path.exists()
                    }
                else:
                    source_info = {
                        'database_type': 'PostgreSQL',
                        'connection_info': 'Remote PostgreSQL'
                    }
                
                # 2. æª¢æŸ¥æ¸¬è©¦æ—¥æœŸçš„æ•¸æ“š
                test_data_info = self.get_test_date_data(cur)
                source_info.update(test_data_info)
                
                # 3. æª¢æŸ¥æ•¸æ“šè¡¨çµæ§‹
                table_info = self.get_table_structure(cur)
                source_info.update(table_info)
                
                return source_info
                
        except Exception as e:
            logger.error(f"âŒ ç²å–æ•¸æ“šæºä¿¡æ¯å¤±æ•—: {e}")
            return {'error': str(e)}
    
    def get_test_date_data(self, cur):
        """ç²å–æ¸¬è©¦æ—¥æœŸçš„æ•¸æ“šè©³æƒ…"""
        try:
            # ç²å–æ¸¬è©¦æ—¥æœŸçš„æ‰€æœ‰æ•¸æ“š
            cur.execute(
                "SELECT * FROM stock_prices WHERE trade_datetime::date = %s ORDER BY trade_datetime;", 
                (self.test_date,)
            )
            all_data = cur.fetchall()
            
            if not all_data:
                return {
                    'test_date': self.test_date,
                    'total_records': 0,
                    'data_available': False
                }
            
            # éæ¿¾äº¤æ˜“æ™‚æ®µæ•¸æ“š (8:45-13:45)
            session_data = [
                c for c in all_data 
                if time(8, 45) <= c['trade_datetime'].time() <= time(13, 45)
            ]
            
            # ç²å–é–‹ç›¤å€é–“æ•¸æ“š (8:46-8:47)
            range_data = [
                c for c in session_data 
                if c['trade_datetime'].time() in [time(8, 46), time(8, 47)]
            ]
            
            # è¨ˆç®—æ•¸æ“šæ‘˜è¦
            if session_data:
                first_record = session_data[0]
                last_record = session_data[-1]
                
                # è¨ˆç®—æ•¸æ“šå“ˆå¸Œå€¼ï¼ˆç”¨æ–¼æ¯”è¼ƒä¸€è‡´æ€§ï¼‰
                data_str = json.dumps([
                    {
                        'time': c['trade_datetime'].isoformat(),
                        'open': float(c['open_price']),
                        'high': float(c['high_price']),
                        'low': float(c['low_price']),
                        'close': float(c['close_price'])
                    } for c in session_data
                ], sort_keys=True)
                data_hash = hashlib.md5(data_str.encode()).hexdigest()
                
                return {
                    'test_date': self.test_date,
                    'total_records': len(all_data),
                    'session_records': len(session_data),
                    'range_records': len(range_data),
                    'data_available': True,
                    'first_time': first_record['trade_datetime'].isoformat(),
                    'last_time': last_record['trade_datetime'].isoformat(),
                    'data_hash': data_hash,
                    'sample_records': {
                        'first': {
                            'time': first_record['trade_datetime'].isoformat(),
                            'open': float(first_record['open_price']),
                            'high': float(first_record['high_price']),
                            'low': float(first_record['low_price']),
                            'close': float(first_record['close_price'])
                        },
                        'last': {
                            'time': last_record['trade_datetime'].isoformat(),
                            'open': float(last_record['open_price']),
                            'high': float(last_record['high_price']),
                            'low': float(last_record['low_price']),
                            'close': float(last_record['close_price'])
                        }
                    }
                }
            else:
                return {
                    'test_date': self.test_date,
                    'total_records': len(all_data),
                    'session_records': 0,
                    'data_available': False
                }
                
        except Exception as e:
            logger.error(f"âŒ ç²å–æ¸¬è©¦æ—¥æœŸæ•¸æ“šå¤±æ•—: {e}")
            return {'test_date_error': str(e)}
    
    def get_table_structure(self, cur):
        """ç²å–æ•¸æ“šè¡¨çµæ§‹ä¿¡æ¯"""
        try:
            if self.use_sqlite:
                cur.execute("PRAGMA table_info(stock_prices);")
                columns = cur.fetchall()
                return {
                    'table_structure': [
                        {
                            'name': col['name'],
                            'type': col['type'],
                            'notnull': bool(col['notnull']),
                            'pk': bool(col['pk'])
                        } for col in columns
                    ]
                }
            else:
                # PostgreSQL è¡¨çµæ§‹æŸ¥è©¢
                cur.execute("""
                    SELECT column_name, data_type, is_nullable 
                    FROM information_schema.columns 
                    WHERE table_name = 'stock_prices'
                    ORDER BY ordinal_position;
                """)
                columns = cur.fetchall()
                return {
                    'table_structure': [
                        {
                            'name': col['column_name'],
                            'type': col['data_type'],
                            'nullable': col['is_nullable'] == 'YES'
                        } for col in columns
                    ]
                }
        except Exception as e:
            logger.error(f"âŒ ç²å–è¡¨çµæ§‹å¤±æ•—: {e}")
            return {'table_structure_error': str(e)}
    
    def run_validation(self):
        """åŸ·è¡Œæ•¸æ“šæºé©—è­‰"""
        logger.info("ğŸš€ é–‹å§‹æ•¸æ“šæºé©—è­‰...")
        
        # åˆå§‹åŒ–æ•¸æ“šæº
        if not self.init_data_source():
            return False
        
        # æ¨¡æ“¬å…©å€‹ç³»çµ±çš„æ•¸æ“šæºæª¢æŸ¥
        logger.info("=" * 60)
        logger.info("æª¢æŸ¥ rev_web_trading_gui.py ä½¿ç”¨çš„æ•¸æ“šæº")
        logger.info("=" * 60)
        rev_web_data = self.get_data_source_info("rev_web_trading_gui")
        
        logger.info("=" * 60)
        logger.info("æª¢æŸ¥ mdd_gui.py ä½¿ç”¨çš„æ•¸æ“šæº")
        logger.info("=" * 60)
        mdd_gui_data = self.get_data_source_info("mdd_gui")
        
        # ä¿å­˜çµæœ
        self.results = {
            'rev_web_trading_gui': rev_web_data,
            'mdd_gui': mdd_gui_data,
            'validation_time': str(date.today())
        }
        
        # æ¯”è¼ƒçµæœ
        self.compare_data_sources()
        
        return True
    
    def compare_data_sources(self):
        """æ¯”è¼ƒå…©å€‹æ•¸æ“šæºçš„ä¸€è‡´æ€§"""
        logger.info("=" * 60)
        logger.info("æ•¸æ“šæºä¸€è‡´æ€§æ¯”è¼ƒ")
        logger.info("=" * 60)
        
        rev_data = self.results['rev_web_trading_gui']
        mdd_data = self.results['mdd_gui']
        
        # æ¯”è¼ƒé—œéµå­—æ®µ
        comparison_fields = [
            'database_type', 'database_path', 'database_size', 'database_mtime',
            'total_records', 'session_records', 'range_records', 'data_hash'
        ]
        
        differences = []
        matches = []
        
        for field in comparison_fields:
            if field in rev_data and field in mdd_data:
                if rev_data[field] == mdd_data[field]:
                    matches.append(field)
                    logger.info(f"âœ… {field}: ä¸€è‡´ ({rev_data[field]})")
                else:
                    differences.append(field)
                    logger.warning(f"âŒ {field}: ä¸ä¸€è‡´")
                    logger.warning(f"   rev_web_trading_gui: {rev_data[field]}")
                    logger.warning(f"   mdd_gui: {mdd_data[field]}")
            else:
                logger.warning(f"âš ï¸ {field}: ç¼ºå°‘æ•¸æ“š")
        
        # ç¸½çµ
        logger.info("=" * 60)
        logger.info("é©—è­‰ç¸½çµ")
        logger.info("=" * 60)
        logger.info(f"âœ… ä¸€è‡´å­—æ®µ: {len(matches)}")
        logger.info(f"âŒ ä¸ä¸€è‡´å­—æ®µ: {len(differences)}")
        
        if differences:
            logger.error("ğŸš¨ ç™¼ç¾æ•¸æ“šæºä¸ä¸€è‡´ï¼")
            logger.error(f"ä¸ä¸€è‡´å­—æ®µ: {differences}")
        else:
            logger.info("ğŸ‰ æ•¸æ“šæºå®Œå…¨ä¸€è‡´ï¼")
        
        # ä¿å­˜è©³ç´°å ±å‘Š
        self.save_report()
    
    def save_report(self):
        """ä¿å­˜é©—è­‰å ±å‘Š"""
        report_file = "ä»»å‹™2_æ•¸æ“šæºé©—è­‰å ±å‘Š.json"
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"ğŸ“‹ é©—è­‰å ±å‘Šå·²ä¿å­˜: {report_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å ±å‘Šå¤±æ•—: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    validator = DataSourceValidator()
    success = validator.run_validation()
    
    if not success:
        sys.exit(1)

if __name__ == "__main__":
    main()
