#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦PostgreSQLåŒ¯å…¥æ€§èƒ½
æ¯”è¼ƒä¸åŒåŒ¯å…¥æ–¹æ³•çš„é€Ÿåº¦
"""

import os
import sys
import logging
import time

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from database.postgres_importer_optimized import PostgreSQLImporterOptimized
from database.postgres_importer import PostgreSQLImporter
from utils.logger import setup_logger, get_logger

# è¨­å®šæ—¥èªŒ
setup_logger()
logger = get_logger(__name__)

def test_original_importer():
    """æ¸¬è©¦åŸå§‹åŒ¯å…¥å™¨"""
    logger.info("ğŸ§ª æ¸¬è©¦åŸå§‹åŒ¯å…¥å™¨...")
    
    try:
        importer = PostgreSQLImporter()
        if not importer.postgres_initialized:
            logger.error("âŒ PostgreSQLæœªåˆå§‹åŒ–")
            return None
        
        start_time = time.time()
        success = importer.import_kline_to_postgres(
            symbol='MTX00',
            kline_type='MINUTE',
            batch_size=1000
        )
        elapsed_time = time.time() - start_time
        
        logger.info(f"ğŸ“Š åŸå§‹åŒ¯å…¥å™¨: {'æˆåŠŸ' if success else 'å¤±æ•—'} (è€—æ™‚: {elapsed_time:.2f}ç§’)")
        return elapsed_time if success else None
        
    except Exception as e:
        logger.error(f"âŒ åŸå§‹åŒ¯å…¥å™¨æ¸¬è©¦å¤±æ•—: {e}")
        return None

def test_optimized_importer():
    """æ¸¬è©¦å„ªåŒ–åŒ¯å…¥å™¨"""
    logger.info("ğŸ§ª æ¸¬è©¦å„ªåŒ–åŒ¯å…¥å™¨...")
    
    try:
        importer = PostgreSQLImporterOptimized()
        if not importer.postgres_initialized:
            logger.error("âŒ PostgreSQLæœªåˆå§‹åŒ–")
            return {}
        
        # æ¸¬è©¦ä¸åŒæ–¹æ³•
        methods = ['copy', 'executemany', 'batch']
        results = {}
        
        for method in methods:
            logger.info(f"ğŸ”¬ æ¸¬è©¦æ–¹æ³•: {method}")
            
            # æ¸…ç©ºè¡¨æ ¼ä»¥ç¢ºä¿æ¸¬è©¦å…¬å¹³æ€§
            clear_table()
            
            start_time = time.time()
            success = importer.import_kline_to_postgres_fast(
                symbol='MTX00',
                kline_type='MINUTE',
                method=method
            )
            elapsed_time = time.time() - start_time
            
            results[method] = {
                'success': success,
                'time': elapsed_time if success else None
            }
            
            logger.info(f"ğŸ“Š {method} æ–¹æ³•: {'æˆåŠŸ' if success else 'å¤±æ•—'} (è€—æ™‚: {elapsed_time:.2f}ç§’)")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ å„ªåŒ–åŒ¯å…¥å™¨æ¸¬è©¦å¤±æ•—: {e}")
        return {}

def clear_table():
    """æ¸…ç©ºstock_pricesè¡¨"""
    try:
        import shared
        with shared.get_conn_cur_from_pool_b(as_dict=False) as (pg_conn, pg_cursor):
            pg_cursor.execute("DELETE FROM stock_prices")
            pg_conn.commit()
        logger.info("ğŸ—‘ï¸ å·²æ¸…ç©ºstock_pricesè¡¨")
    except Exception as e:
        logger.warning(f"âš ï¸ æ¸…ç©ºè¡¨æ ¼å¤±æ•—: {e}")

def get_data_count():
    """å–å¾—è³‡æ–™ç­†æ•¸"""
    try:
        import shared
        with shared.get_conn_cur_from_pool_b(as_dict=False) as (pg_conn, pg_cursor):
            pg_cursor.execute("SELECT COUNT(*) FROM stock_prices")
            count = pg_cursor.fetchone()[0]
        return count
    except Exception as e:
        logger.error(f"âŒ å–å¾—è³‡æ–™ç­†æ•¸å¤±æ•—: {e}")
        return 0

def analyze_table_structure():
    """åˆ†æè¡¨æ ¼çµæ§‹å’Œç´¢å¼•"""
    logger.info("ğŸ” åˆ†æstock_pricesè¡¨çµæ§‹...")
    
    try:
        import shared
        with shared.get_conn_cur_from_pool_b(as_dict=True) as (pg_conn, pg_cursor):
            # æª¢æŸ¥è¡¨æ ¼çµæ§‹
            pg_cursor.execute("""
                SELECT column_name, data_type, is_nullable
                FROM information_schema.columns
                WHERE table_name = 'stock_prices'
                ORDER BY ordinal_position
            """)
            columns = pg_cursor.fetchall()
            
            logger.info("ğŸ“‹ è¡¨æ ¼æ¬„ä½:")
            for col in columns:
                logger.info(f"  - {col['column_name']}: {col['data_type']} ({'NULL' if col['is_nullable'] == 'YES' else 'NOT NULL'})")
            
            # æª¢æŸ¥ç´¢å¼•
            pg_cursor.execute("""
                SELECT indexname, indexdef
                FROM pg_indexes
                WHERE tablename = 'stock_prices'
            """)
            indexes = pg_cursor.fetchall()
            
            logger.info("ğŸ”‘ è¡¨æ ¼ç´¢å¼•:")
            for idx in indexes:
                logger.info(f"  - {idx['indexname']}: {idx['indexdef']}")
            
            # æª¢æŸ¥ç´„æŸ
            pg_cursor.execute("""
                SELECT constraint_name, constraint_type
                FROM information_schema.table_constraints
                WHERE table_name = 'stock_prices'
            """)
            constraints = pg_cursor.fetchall()
            
            logger.info("ğŸ”’ è¡¨æ ¼ç´„æŸ:")
            for const in constraints:
                logger.info(f"  - {const['constraint_name']}: {const['constraint_type']}")
                
    except Exception as e:
        logger.error(f"âŒ åˆ†æè¡¨æ ¼çµæ§‹å¤±æ•—: {e}")

def suggest_optimizations():
    """å»ºè­°å„ªåŒ–æ–¹æ¡ˆ"""
    logger.info("ğŸ’¡ æ€§èƒ½å„ªåŒ–å»ºè­°:")
    
    suggestions = [
        "1. ä½¿ç”¨COPYå‘½ä»¤é€²è¡Œå¤§é‡è³‡æ–™åŒ¯å…¥ (æœ€å¿«)",
        "2. ä½¿ç”¨psycopg2.extras.execute_valuesé€²è¡Œæ‰¹é‡æ’å…¥",
        "3. åœ¨åŒ¯å…¥å‰æš«æ™‚ç§»é™¤éå¿…è¦ç´¢å¼•",
        "4. èª¿æ•´PostgreSQLè¨­å®š: synchronous_commit = OFF",
        "5. å¢åŠ work_memè¨­å®šä»¥æå‡æ’åºå’Œé›œæ¹Šæ“ä½œæ€§èƒ½",
        "6. ä½¿ç”¨è¼ƒå¤§çš„batch_size (5000-10000)",
        "7. åœ¨åŒ¯å…¥å®Œæˆå¾Œå†å»ºç«‹ç´¢å¼•",
        "8. è€ƒæ…®ä½¿ç”¨UNLOGGEDè¡¨æ ¼é€²è¡Œè‡¨æ™‚åŒ¯å…¥"
    ]
    
    for suggestion in suggestions:
        logger.info(f"  {suggestion}")

def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸš€ é–‹å§‹PostgreSQLåŒ¯å…¥æ€§èƒ½æ¸¬è©¦...")
    
    # åˆ†æè¡¨æ ¼çµæ§‹
    analyze_table_structure()
    
    # æª¢æŸ¥åˆå§‹è³‡æ–™ç­†æ•¸
    initial_count = get_data_count()
    logger.info(f"ğŸ“Š åˆå§‹è³‡æ–™ç­†æ•¸: {initial_count}")
    
    # æ¸…ç©ºè¡¨æ ¼
    clear_table()
    
    # æ¸¬è©¦å„ªåŒ–åŒ¯å…¥å™¨
    logger.info("\n" + "="*80)
    logger.info("æ¸¬è©¦å„ªåŒ–åŒ¯å…¥å™¨")
    logger.info("="*80)
    
    optimized_results = test_optimized_importer()
    
    # é¡¯ç¤ºçµæœæ¯”è¼ƒ
    logger.info("\n" + "="*80)
    logger.info("æ€§èƒ½æ¸¬è©¦çµæœç¸½çµ")
    logger.info("="*80)
    
    if optimized_results:
        logger.info("ğŸ† å„ªåŒ–åŒ¯å…¥å™¨çµæœ:")
        fastest_method = None
        fastest_time = float('inf')
        
        for method, result in optimized_results.items():
            if result['success'] and result['time']:
                logger.info(f"  {method}: {result['time']:.2f}ç§’")
                if result['time'] < fastest_time:
                    fastest_time = result['time']
                    fastest_method = method
            else:
                logger.info(f"  {method}: å¤±æ•—")
        
        if fastest_method:
            logger.info(f"ğŸ¥‡ æœ€å¿«æ–¹æ³•: {fastest_method} ({fastest_time:.2f}ç§’)")
            
            # è¨ˆç®—æ€§èƒ½æå‡
            data_count = get_data_count()
            if data_count > 0:
                speed = data_count / fastest_time
                logger.info(f"ğŸ“ˆ åŒ¯å…¥é€Ÿåº¦: {speed:.0f} ç­†/ç§’")
                
                # èˆ‡5åˆ†é˜æ¯”è¼ƒ
                original_time = 5 * 60  # 5åˆ†é˜
                improvement = original_time / fastest_time
                logger.info(f"ğŸš€ æ€§èƒ½æå‡: {improvement:.1f}å€ (å¾5åˆ†é˜ç¸®çŸ­åˆ°{fastest_time:.1f}ç§’)")
    
    # æä¾›å„ªåŒ–å»ºè­°
    logger.info("\n" + "="*80)
    suggest_optimizations()
    
    logger.info("\nâœ… æ€§èƒ½æ¸¬è©¦å®Œæˆï¼")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"âŒ æ¸¬è©¦ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
        sys.exit(1)
