#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IDä¸€è‡´æ€§æ·±åº¦åˆ†æå·¥å…·
å°ˆé–€æª¢æ¸¬group_idç›¸é—œçš„æ··äº‚ä½¿ç”¨å•é¡Œï¼ŒåŒ…æ‹¬ä¸»éµID vs é‚è¼¯IDçš„æ··ç”¨
"""

import os
import sys
import re
import ast
import sqlite3
from datetime import datetime
from typing import Dict, List, Tuple, Set

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

class IDConsistencyDeepAnalyzer:
    """IDä¸€è‡´æ€§æ·±åº¦åˆ†æå™¨"""
    
    def __init__(self):
        self.issues = []
        self.critical_issues = []
        self.warning_issues = []
        
        # é—œéµæ–‡ä»¶åˆ—è¡¨
        self.key_files = [
            "multi_group_database.py",
            "multi_group_position_manager.py", 
            "cumulative_profit_protection_manager.py",
            "optimized_risk_manager.py",
            "stop_loss_executor.py",
            "simplified_order_tracker.py"
        ]
        
        print("ğŸ” IDä¸€è‡´æ€§æ·±åº¦åˆ†æå·¥å…·")
        print("=" * 60)
        print("ğŸ¯ åˆ†æç›®æ¨™:")
        print("  1. group_id vs ä¸»éµIDæ··ç”¨å•é¡Œ")
        print("  2. position_id vs ä¸»éµIDæ··ç”¨å•é¡Œ")
        print("  3. è³‡æ–™åº«æŸ¥è©¢é‚è¼¯ä¸€è‡´æ€§")
        print("  4. å‡½æ•¸åƒæ•¸å‘½åæ··äº‚")
        print("  5. è®Šæ•¸å‘½åä¸ä¸€è‡´")
        print("=" * 60)
    
    def analyze_database_schema_consistency(self):
        """åˆ†æè³‡æ–™åº«è¡¨çµæ§‹ä¸€è‡´æ€§"""
        print("\nğŸ” åˆ†æè³‡æ–™åº«è¡¨çµæ§‹ä¸€è‡´æ€§")
        print("-" * 50)
        
        # æª¢æŸ¥æ­£å¼æ©Ÿå’Œè™›æ“¬æ¸¬è©¦æ©Ÿ
        databases = [
            ("æ­£å¼æ©Ÿ", "multi_group_strategy.db"),
            ("è™›æ“¬æ¸¬è©¦æ©Ÿ", "test_virtual_strategy.db")
        ]
        
        schema_issues = []
        
        for env_name, db_path in databases:
            if not os.path.exists(db_path):
                schema_issues.append(f"âŒ {env_name}è³‡æ–™åº«ä¸å­˜åœ¨: {db_path}")
                continue
            
            try:
                conn = sqlite3.connect(db_path)
                cursor = conn.cursor()
                
                print(f"\nğŸ“Š {env_name} ({db_path}):")
                
                # æª¢æŸ¥strategy_groupsè¡¨çµæ§‹
                cursor.execute("PRAGMA table_info(strategy_groups)")
                sg_columns = {row[1]: row[2] for row in cursor.fetchall()}
                
                # æª¢æŸ¥position_recordsè¡¨çµæ§‹
                cursor.execute("PRAGMA table_info(position_records)")
                pr_columns = {row[1]: row[2] for row in cursor.fetchall()}
                
                print(f"  strategy_groupsæ¬„ä½: {list(sg_columns.keys())}")
                print(f"  position_recordsæ¬„ä½: {list(pr_columns.keys())}")
                
                # æª¢æŸ¥é—œéµæ¬„ä½
                critical_fields = {
                    'strategy_groups': ['id', 'group_id', 'date'],
                    'position_records': ['id', 'group_id', 'lot_id']
                }
                
                for table, fields in critical_fields.items():
                    table_columns = sg_columns if table == 'strategy_groups' else pr_columns
                    for field in fields:
                        if field not in table_columns:
                            schema_issues.append(f"âŒ {env_name}.{table}ç¼ºå°‘æ¬„ä½: {field}")
                        else:
                            print(f"  âœ… {table}.{field}: {table_columns[field]}")
                
                # æª¢æŸ¥å¤–éµé—œä¿‚é‚è¼¯
                cursor.execute('''
                    SELECT COUNT(*) FROM position_records pr
                    LEFT JOIN strategy_groups sg ON pr.group_id = sg.group_id AND sg.date = date('now')
                    WHERE sg.id IS NULL AND pr.status = 'ACTIVE'
                ''')
                
                orphaned_positions = cursor.fetchone()[0]
                if orphaned_positions > 0:
                    schema_issues.append(f"âš ï¸ {env_name}æœ‰{orphaned_positions}å€‹å­¤ç«‹éƒ¨ä½ï¼ˆæ‰¾ä¸åˆ°å°æ‡‰ç­–ç•¥çµ„ï¼‰")
                
                conn.close()
                
            except Exception as e:
                schema_issues.append(f"âŒ {env_name}è³‡æ–™åº«æª¢æŸ¥å¤±æ•—: {e}")
        
        if schema_issues:
            print(f"\nğŸš¨ ç™¼ç¾è³‡æ–™åº«çµæ§‹å•é¡Œ:")
            for issue in schema_issues:
                print(f"  {issue}")
            self.critical_issues.extend(schema_issues)
        else:
            print(f"\nâœ… è³‡æ–™åº«çµæ§‹ä¸€è‡´æ€§æª¢æŸ¥é€šé")
    
    def analyze_sql_queries_in_code(self):
        """åˆ†æä»£ç¢¼ä¸­çš„SQLæŸ¥è©¢é‚è¼¯"""
        print("\nğŸ” åˆ†æä»£ç¢¼ä¸­çš„SQLæŸ¥è©¢é‚è¼¯")
        print("-" * 50)
        
        sql_issues = []
        
        for filename in self.key_files:
            if not os.path.exists(filename):
                continue
            
            print(f"\nğŸ“„ åˆ†ææ–‡ä»¶: {filename}")
            
            with open(filename, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')
            
            # æŸ¥æ‰¾SQLæŸ¥è©¢
            sql_patterns = [
                r'WHERE\s+group_id\s*=\s*\?',
                r'WHERE\s+id\s*=\s*\?',
                r'WHERE\s+position_id\s*=\s*\?',
                r'SELECT.*FROM\s+strategy_groups',
                r'SELECT.*FROM\s+position_records',
                r'JOIN.*strategy_groups.*ON',
                r'JOIN.*position_records.*ON'
            ]
            
            for i, line in enumerate(lines, 1):
                line_stripped = line.strip()
                if not line_stripped or line_stripped.startswith('#'):
                    continue
                
                for pattern in sql_patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        # æª¢æŸ¥æ˜¯å¦æœ‰æ½›åœ¨çš„IDæ··ç”¨å•é¡Œ
                        if 'WHERE group_id = ?' in line and 'strategy_groups' in line:
                            # é€™æ˜¯æ­£ç¢ºçš„ç”¨æ³•
                            continue
                        elif 'WHERE id = ?' in line and 'strategy_groups' in line:
                            # æª¢æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦æ˜ç¢ºé€™æ˜¯ä¸»éµæŸ¥è©¢
                            context_lines = lines[max(0, i-3):min(len(lines), i+3)]
                            context = '\n'.join(context_lines)
                            if 'db_id' not in context.lower() and 'primary' not in context.lower():
                                sql_issues.append(f"âš ï¸ {filename}:{i} å¯èƒ½çš„IDæ··ç”¨: {line_stripped}")
                        
                        # æª¢æŸ¥JOINé‚è¼¯
                        if 'JOIN' in line.upper() and 'group_id' in line:
                            if 'pr.group_id = sg.group_id' not in line:
                                sql_issues.append(f"âš ï¸ {filename}:{i} JOINé‚è¼¯å¯èƒ½æœ‰å•é¡Œ: {line_stripped}")
        
        if sql_issues:
            print(f"\nğŸš¨ ç™¼ç¾SQLæŸ¥è©¢å•é¡Œ:")
            for issue in sql_issues:
                print(f"  {issue}")
            self.warning_issues.extend(sql_issues)
        else:
            print(f"\nâœ… SQLæŸ¥è©¢é‚è¼¯æª¢æŸ¥é€šé")
    
    def analyze_function_parameters(self):
        """åˆ†æå‡½æ•¸åƒæ•¸å‘½åä¸€è‡´æ€§"""
        print("\nğŸ” åˆ†æå‡½æ•¸åƒæ•¸å‘½åä¸€è‡´æ€§")
        print("-" * 50)
        
        param_issues = []
        
        for filename in self.key_files:
            if not os.path.exists(filename):
                continue
            
            print(f"\nğŸ“„ åˆ†ææ–‡ä»¶: {filename}")
            
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ä½¿ç”¨ASTè§£æPythonä»£ç¢¼
                tree = ast.parse(content)
                
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        func_name = node.name
                        
                        # æª¢æŸ¥å‡½æ•¸åƒæ•¸
                        for arg in node.args.args:
                            arg_name = arg.arg
                            
                            # æª¢æŸ¥å¯èƒ½çš„IDåƒæ•¸æ··äº‚
                            if 'group' in arg_name and 'id' in arg_name:
                                if arg_name not in ['group_id', 'group_db_id', 'logical_group_id', 'group_pk']:
                                    param_issues.append(f"âš ï¸ {filename} {func_name}() åƒæ•¸å‘½åä¸è¦ç¯„: {arg_name}")
                            
                            if 'position' in arg_name and 'id' in arg_name:
                                if arg_name not in ['position_id', 'position_pk']:
                                    param_issues.append(f"âš ï¸ {filename} {func_name}() åƒæ•¸å‘½åä¸è¦ç¯„: {arg_name}")
                            
                            # æª¢æŸ¥ç°¡å–®çš„'id'åƒæ•¸
                            if arg_name == 'id':
                                param_issues.append(f"âš ï¸ {filename} {func_name}() ä½¿ç”¨æ¨¡ç³Šåƒæ•¸å: id (å»ºè­°ä½¿ç”¨æ›´æ˜ç¢ºçš„åç¨±)")
                
            except Exception as e:
                param_issues.append(f"âŒ {filename} ASTè§£æå¤±æ•—: {e}")
        
        if param_issues:
            print(f"\nğŸš¨ ç™¼ç¾å‡½æ•¸åƒæ•¸å•é¡Œ:")
            for issue in param_issues:
                print(f"  {issue}")
            self.warning_issues.extend(param_issues)
        else:
            print(f"\nâœ… å‡½æ•¸åƒæ•¸å‘½åæª¢æŸ¥é€šé")
    
    def analyze_variable_naming_consistency(self):
        """åˆ†æè®Šæ•¸å‘½åä¸€è‡´æ€§"""
        print("\nğŸ” åˆ†æè®Šæ•¸å‘½åä¸€è‡´æ€§")
        print("-" * 50)
        
        var_issues = []
        
        # å®šç¾©æ¨™æº–å‘½åæ¨¡å¼
        standard_patterns = {
            'group_id': r'\bgroup_id\b',
            'group_db_id': r'\bgroup_db_id\b',
            'group_pk': r'\bgroup_pk\b',
            'logical_group_id': r'\blogical_group_id\b',
            'position_id': r'\bposition_id\b',
            'position_pk': r'\bposition_pk\b'
        }
        
        # æª¢æŸ¥éæ¨™æº–å‘½å
        non_standard_patterns = [
            r'\bgid\b', r'\bgrp_id\b', r'\bgroup_identifier\b',
            r'\bpos_id\b', r'\bposition_identifier\b',
            r'\bid\s*=', r'\bid\s*\)', r'\bid\s*,'
        ]
        
        for filename in self.key_files:
            if not os.path.exists(filename):
                continue
            
            print(f"\nğŸ“„ åˆ†ææ–‡ä»¶: {filename}")
            
            with open(filename, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')
            
            # æª¢æŸ¥éæ¨™æº–å‘½å
            for i, line in enumerate(lines, 1):
                line_stripped = line.strip()
                if not line_stripped or line_stripped.startswith('#'):
                    continue
                
                for pattern in non_standard_patterns:
                    if re.search(pattern, line):
                        # æ’é™¤ä¸€äº›åˆç†çš„ä½¿ç”¨æƒ…æ³
                        if 'PRIMARY KEY' in line.upper() or 'AUTOINCREMENT' in line.upper():
                            continue
                        if 'def ' in line and pattern == r'\bid\s*\)':
                            continue
                        
                        var_issues.append(f"âš ï¸ {filename}:{i} éæ¨™æº–å‘½å: {line_stripped}")
        
        if var_issues:
            print(f"\nğŸš¨ ç™¼ç¾è®Šæ•¸å‘½åå•é¡Œ:")
            for issue in var_issues[:10]:  # åªé¡¯ç¤ºå‰10å€‹
                print(f"  {issue}")
            if len(var_issues) > 10:
                print(f"  ... é‚„æœ‰{len(var_issues)-10}å€‹é¡ä¼¼å•é¡Œ")
            self.warning_issues.extend(var_issues)
        else:
            print(f"\nâœ… è®Šæ•¸å‘½åä¸€è‡´æ€§æª¢æŸ¥é€šé")
    
    def check_critical_id_usage_patterns(self):
        """æª¢æŸ¥é—œéµIDä½¿ç”¨æ¨¡å¼"""
        print("\nğŸ” æª¢æŸ¥é—œéµIDä½¿ç”¨æ¨¡å¼")
        print("-" * 50)
        
        critical_patterns = []
        
        # æª¢æŸ¥å¯èƒ½çš„é—œéµå•é¡Œæ¨¡å¼
        problem_patterns = [
            {
                'pattern': r'get_strategy_group_info\(\s*(\w+)\s*\)',
                'description': 'æª¢æŸ¥get_strategy_group_infoçš„åƒæ•¸æ˜¯å¦ç‚ºé‚è¼¯group_id',
                'file_focus': ['multi_group_position_manager.py', 'multi_group_database.py']
            },
            {
                'pattern': r'WHERE\s+group_id\s*=\s*\?\s*.*strategy_groups',
                'description': 'æª¢æŸ¥strategy_groupsè¡¨çš„group_idæŸ¥è©¢',
                'file_focus': ['multi_group_database.py']
            },
            {
                'pattern': r'WHERE\s+id\s*=\s*\?\s*.*strategy_groups',
                'description': 'æª¢æŸ¥strategy_groupsè¡¨çš„ä¸»éµæŸ¥è©¢',
                'file_focus': ['multi_group_database.py']
            }
        ]
        
        for pattern_info in problem_patterns:
            pattern = pattern_info['pattern']
            description = pattern_info['description']
            files_to_check = pattern_info.get('file_focus', self.key_files)
            
            print(f"\nğŸ” {description}")
            
            for filename in files_to_check:
                if not os.path.exists(filename):
                    continue
                
                with open(filename, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = content.split('\n')
                
                matches = re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    # æ‰¾åˆ°åŒ¹é…çš„è¡Œè™Ÿ
                    line_num = content[:match.start()].count('\n') + 1
                    line_content = lines[line_num - 1].strip()
                    
                    print(f"  ğŸ“ {filename}:{line_num} {line_content}")
                    
                    # åˆ†æä¸Šä¸‹æ–‡
                    context_start = max(0, line_num - 3)
                    context_end = min(len(lines), line_num + 3)
                    context = lines[context_start:context_end]
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰æ˜ç¢ºçš„è¨»é‡‹èªªæ˜
                    has_clear_comment = any('ä¸»éµ' in line or 'DB_ID' in line or 'logical' in line 
                                          for line in context)
                    
                    if not has_clear_comment:
                        critical_patterns.append(f"âš ï¸ {filename}:{line_num} {description} - ç¼ºå°‘æ˜ç¢ºè¨»é‡‹")
        
        if critical_patterns:
            print(f"\nğŸš¨ ç™¼ç¾é—œéµIDä½¿ç”¨å•é¡Œ:")
            for issue in critical_patterns:
                print(f"  {issue}")
            self.critical_issues.extend(critical_patterns)
        else:
            print(f"\nâœ… é—œéµIDä½¿ç”¨æ¨¡å¼æª¢æŸ¥é€šé")
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç¶œåˆåˆ†æå ±å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š IDä¸€è‡´æ€§æ·±åº¦åˆ†æå ±å‘Š")
        print("=" * 60)
        
        total_issues = len(self.critical_issues) + len(self.warning_issues)
        
        print(f"åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"åˆ†ææ–‡ä»¶æ•¸: {len(self.key_files)}")
        print(f"ç™¼ç¾å•é¡Œç¸½æ•¸: {total_issues}")
        print(f"  é—œéµå•é¡Œ: {len(self.critical_issues)}")
        print(f"  è­¦å‘Šå•é¡Œ: {len(self.warning_issues)}")
        
        if self.critical_issues:
            print(f"\nğŸš¨ é—œéµå•é¡Œ (éœ€è¦ç«‹å³ä¿®å¾©):")
            for i, issue in enumerate(self.critical_issues, 1):
                print(f"  {i}. {issue}")
        
        if self.warning_issues:
            print(f"\nâš ï¸ è­¦å‘Šå•é¡Œ (å»ºè­°ä¿®å¾©):")
            for i, issue in enumerate(self.warning_issues[:5], 1):  # åªé¡¯ç¤ºå‰5å€‹
                print(f"  {i}. {issue}")
            if len(self.warning_issues) > 5:
                print(f"  ... é‚„æœ‰{len(self.warning_issues)-5}å€‹è­¦å‘Šå•é¡Œ")
        
        # ç”Ÿæˆä¿®å¾©å»ºè­°
        print(f"\nğŸ’¡ ä¿®å¾©å»ºè­°:")
        if len(self.critical_issues) > 0:
            print("  1. ğŸ”§ ç«‹å³ä¿®å¾©é—œéµå•é¡Œï¼Œç‰¹åˆ¥æ˜¯IDæ··ç”¨å•é¡Œ")
            print("  2. ğŸ“‹ çµ±ä¸€ä½¿ç”¨æ˜ç¢ºçš„åƒæ•¸å‘½å (group_id vs group_db_id)")
            print("  3. ğŸ” ç‚ºæ‰€æœ‰IDç›¸é—œæŸ¥è©¢æ·»åŠ æ˜ç¢ºè¨»é‡‹")
        
        if len(self.warning_issues) > 0:
            print("  4. âš ï¸ é€æ­¥ä¿®å¾©è­¦å‘Šå•é¡Œï¼Œæé«˜ä»£ç¢¼å¯ç¶­è­·æ€§")
            print("  5. ğŸ“ å»ºç«‹IDå‘½åè¦ç¯„æ–‡æª”")
        
        if total_issues == 0:
            print("  ğŸ‰ IDä¸€è‡´æ€§è‰¯å¥½ï¼Œç„¡éœ€ä¿®å¾©ï¼")
        
        # ä¿å­˜è©³ç´°å ±å‘Š
        report_file = f"id_consistency_deep_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("IDä¸€è‡´æ€§æ·±åº¦åˆ†æè©³ç´°å ±å‘Š\n")
            f.write("=" * 60 + "\n")
            f.write(f"åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ç¸½å•é¡Œæ•¸: {total_issues}\n\n")
            
            f.write("é—œéµå•é¡Œ:\n")
            for issue in self.critical_issues:
                f.write(f"  {issue}\n")
            
            f.write("\nè­¦å‘Šå•é¡Œ:\n")
            for issue in self.warning_issues:
                f.write(f"  {issue}\n")
        
        print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {report_file}")
        
        return total_issues == 0
    
    def run_deep_analysis(self):
        """é‹è¡Œæ·±åº¦åˆ†æ"""
        print("ğŸš€ é–‹å§‹IDä¸€è‡´æ€§æ·±åº¦åˆ†æ")
        
        # åŸ·è¡Œå„é …åˆ†æ
        self.analyze_database_schema_consistency()
        self.analyze_sql_queries_in_code()
        self.analyze_function_parameters()
        self.analyze_variable_naming_consistency()
        self.check_critical_id_usage_patterns()
        
        # ç”Ÿæˆç¶œåˆå ±å‘Š
        success = self.generate_comprehensive_report()
        
        return success

if __name__ == "__main__":
    analyzer = IDConsistencyDeepAnalyzer()
    success = analyzer.run_deep_analysis()
    
    if success:
        print("\nğŸ‰ IDä¸€è‡´æ€§æ·±åº¦åˆ†æå®Œæˆï¼šæœªç™¼ç¾åš´é‡å•é¡Œï¼")
    else:
        print("\nâš ï¸ IDä¸€è‡´æ€§æ·±åº¦åˆ†æå®Œæˆï¼šç™¼ç¾éœ€è¦ä¿®å¾©çš„å•é¡Œ")
    
    exit(0 if success else 1)
