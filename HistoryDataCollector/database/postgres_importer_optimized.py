#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PostgreSQLåŒ¯å…¥å™¨ - å„ªåŒ–ç‰ˆæœ¬
é‡å°1140ç­†è³‡æ–™å¾5åˆ†é˜å„ªåŒ–åˆ°å¹¾ç§’é˜
"""

import os
import sys
import logging
import sqlite3
import time
from datetime import datetime
from decimal import Decimal
import tempfile
import csv

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å°å…¥æ‚¨çš„è³‡æ–™åº«æ¨¡çµ„
try:
    from app_setup import init_all_db_pools
    import shared
    HAS_POSTGRES_MODULES = True
except ImportError as e:
    HAS_POSTGRES_MODULES = False
    print(f"âš ï¸ ç„¡æ³•å°å…¥PostgreSQLæ¨¡çµ„: {e}")

from history_config import DATABASE_PATH

logger = logging.getLogger(__name__)

class PostgreSQLImporterOptimized:
    """PostgreSQLåŒ¯å…¥å™¨ - å„ªåŒ–ç‰ˆæœ¬"""

    def __init__(self):
        """åˆå§‹åŒ–åŒ¯å…¥å™¨"""
        self.sqlite_db_path = "data/history_data.db"
        self.postgres_initialized = False
        
        if not HAS_POSTGRES_MODULES:
            logger.error("âŒ PostgreSQLæ¨¡çµ„æœªæ‰¾åˆ°ï¼Œç„¡æ³•é€²è¡ŒåŒ¯å…¥")
            return
        
        # åˆå§‹åŒ–PostgreSQLé€£ç·šæ± 
        try:
            init_all_db_pools()
            self.postgres_initialized = True
            logger.info("âœ… PostgreSQLé€£ç·šæ± åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ PostgreSQLé€£ç·šæ± åˆå§‹åŒ–å¤±æ•—: {e}")

    def import_kline_to_postgres_fast(self, symbol='MTX00', kline_type='MINUTE', method='copy'):
        """
        å¿«é€ŸåŒ¯å…¥Kç·šè³‡æ–™åˆ°PostgreSQL
        
        Args:
            symbol: å•†å“ä»£ç¢¼
            kline_type: Kç·šé¡å‹
            method: åŒ¯å…¥æ–¹æ³• ('copy', 'batch', 'executemany')
        """
        if not self.postgres_initialized:
            logger.error("âŒ PostgreSQLæœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŒ¯å…¥")
            return False

        total_start_time = time.time()
        logger.info(f"ğŸš€ é–‹å§‹å¿«é€ŸåŒ¯å…¥ {symbol} {kline_type} è³‡æ–™ (æ–¹æ³•: {method})")

        try:
            # 1. è®€å–SQLiteè³‡æ–™
            data_load_start = time.time()
            sqlite_data = self._load_sqlite_data(symbol, kline_type)
            if not sqlite_data:
                return False
            
            data_load_time = time.time() - data_load_start
            logger.info(f"ğŸ“Š SQLiteè³‡æ–™è®€å–å®Œæˆ: {len(sqlite_data)} ç­† (è€—æ™‚: {data_load_time:.2f}ç§’)")

            # 2. è½‰æ›è³‡æ–™æ ¼å¼
            convert_start = time.time()
            converted_data = self._convert_data_batch(sqlite_data)
            convert_time = time.time() - convert_start
            logger.info(f"ğŸ”„ è³‡æ–™è½‰æ›å®Œæˆ: {len(converted_data)} ç­† (è€—æ™‚: {convert_time:.2f}ç§’)")

            if not converted_data:
                logger.warning("âš ï¸ æ²’æœ‰æœ‰æ•ˆè³‡æ–™å¯åŒ¯å…¥")
                return False

            # 3. æ ¹æ“šæ–¹æ³•é¸æ“‡åŒ¯å…¥ç­–ç•¥
            import_start = time.time()
            if method == 'copy':
                success = self._import_using_copy_optimized(converted_data)
            elif method == 'batch':
                success = self._import_using_batch_optimized(converted_data)
            else:  # executemany
                success = self._import_using_executemany_optimized(converted_data)
            
            import_time = time.time() - import_start
            total_time = time.time() - total_start_time

            if success:
                logger.info(f"âœ… åŒ¯å…¥å®Œæˆï¼")
                logger.info(f"ğŸ“Š æ€§èƒ½çµ±è¨ˆ:")
                logger.info(f"  - è³‡æ–™ç­†æ•¸: {len(converted_data)}")
                logger.info(f"  - ç¸½è€—æ™‚: {total_time:.2f}ç§’")
                logger.info(f"  - åŒ¯å…¥è€—æ™‚: {import_time:.2f}ç§’")
                logger.info(f"  - å¹³å‡é€Ÿåº¦: {len(converted_data)/total_time:.0f} ç­†/ç§’")
                logger.info(f"  - åŒ¯å…¥é€Ÿåº¦: {len(converted_data)/import_time:.0f} ç­†/ç§’")
            
            return success

        except Exception as e:
            logger.error(f"âŒ å¿«é€ŸåŒ¯å…¥å¤±æ•—: {e}", exc_info=True)
            return False

    def _load_sqlite_data(self, symbol, kline_type):
        """è¼‰å…¥SQLiteè³‡æ–™"""
        try:
            sqlite_conn = sqlite3.connect(self.sqlite_db_path)
            sqlite_conn.row_factory = sqlite3.Row
            
            query_sql = """
                SELECT * FROM kline_data
                WHERE symbol = ? AND kline_type = ?
                ORDER BY trade_date, trade_time
            """
            
            cursor = sqlite_conn.execute(query_sql, (symbol, kline_type))
            all_rows = cursor.fetchall()
            sqlite_conn.close()
            
            if len(all_rows) == 0:
                logger.warning(f"âš ï¸ æ²’æœ‰æ‰¾åˆ° {symbol} {kline_type} è³‡æ–™")
                return None
            
            return [dict(row) for row in all_rows]
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥SQLiteè³‡æ–™å¤±æ•—: {e}")
            return None

    def _convert_data_batch(self, sqlite_data):
        """æ‰¹é‡è½‰æ›è³‡æ–™æ ¼å¼"""
        converted_data = []
        
        for row_data in sqlite_data:
            try:
                # è§£ææ—¥æœŸæ™‚é–“
                date_str = str(row_data['trade_date']).strip()
                
                # ç›´æ¥è§£ææ—¥æœŸæ™‚é–“å­—ä¸²
                try:
                    trade_datetime = datetime.strptime(date_str, '%Y/%m/%d %H:%M')
                except ValueError:
                    try:
                        trade_datetime = datetime.strptime(date_str, '%Y/%m/%d')
                        trade_datetime = trade_datetime.replace(hour=13, minute=45, second=0)
                    except ValueError:
                        logger.warning(f"âš ï¸ ç„¡æ³•è§£ææ—¥æœŸæ ¼å¼: '{date_str}'")
                        continue

                # è½‰æ›åƒ¹æ ¼è³‡æ–™
                open_price = Decimal(str(row_data['open_price']))
                high_price = Decimal(str(row_data['high_price']))
                low_price = Decimal(str(row_data['low_price']))
                close_price = Decimal(str(row_data['close_price']))
                volume = row_data['volume'] or 0
                price_change = Decimal('0.00')

                converted_data.append({
                    'trade_datetime': trade_datetime,
                    'open_price': open_price,
                    'high_price': high_price,
                    'low_price': low_price,
                    'close_price': close_price,
                    'price_change': price_change,
                    'percentage_change': Decimal('0.0000'),
                    'volume': volume
                })
                
            except Exception as e:
                logger.warning(f"âš ï¸ è½‰æ›è³‡æ–™å¤±æ•—: {e}")
                continue
        
        return converted_data

    def _import_using_copy_optimized(self, converted_data):
        """ä½¿ç”¨COPYå‘½ä»¤è¶…é«˜é€ŸåŒ¯å…¥"""
        try:
            logger.info("âš¡ ä½¿ç”¨COPYå‘½ä»¤é€²è¡Œè¶…é«˜é€ŸåŒ¯å…¥...")
            
            # å‰µå»ºè‡¨æ™‚CSVæª”æ¡ˆ
            temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8')
            csv_writer = csv.writer(temp_file)
            
            # å¯«å…¥è³‡æ–™
            for data in converted_data:
                csv_writer.writerow([
                    data['trade_datetime'].strftime('%Y-%m-%d %H:%M:%S'),
                    str(data['open_price']),
                    str(data['high_price']),
                    str(data['low_price']),
                    str(data['close_price']),
                    str(data['price_change']),
                    str(data['percentage_change']),
                    data['volume']
                ])
            
            temp_file.close()
            
            # ä½¿ç”¨COPYå‘½ä»¤åŒ¯å…¥
            with shared.get_conn_cur_from_pool_b(as_dict=False) as (pg_conn, pg_cursor):
                with open(temp_file.name, 'r', encoding='utf-8') as f:
                    pg_cursor.copy_expert("""
                        COPY stock_prices (
                            trade_datetime, open_price, high_price, low_price,
                            close_price, price_change, percentage_change, volume
                        ) FROM STDIN WITH CSV
                    """, f)
                
                pg_conn.commit()
            
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            os.unlink(temp_file.name)
            logger.info("âœ… COPYå‘½ä»¤åŒ¯å…¥å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ COPYå‘½ä»¤åŒ¯å…¥å¤±æ•—: {e}")
            return False

    def _import_using_batch_optimized(self, converted_data):
        """ä½¿ç”¨æ‰¹é‡æ’å…¥å„ªåŒ–åŒ¯å…¥"""
        try:
            logger.info("âš¡ ä½¿ç”¨æ‰¹é‡æ’å…¥é€²è¡Œå„ªåŒ–åŒ¯å…¥...")
            
            with shared.get_conn_cur_from_pool_b(as_dict=False) as (pg_conn, pg_cursor):
                # æ€§èƒ½å„ªåŒ–è¨­å®š
                pg_cursor.execute("SET synchronous_commit = OFF")
                pg_cursor.execute("SET work_mem = '256MB'")
                
                # æº–å‚™SQLå’Œè³‡æ–™
                insert_sql = """
                    INSERT INTO stock_prices (
                        trade_datetime, open_price, high_price, low_price,
                        close_price, price_change, percentage_change, volume
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (trade_datetime) DO NOTHING
                """
                
                values_list = []
                for data in converted_data:
                    values_list.append((
                        data['trade_datetime'],
                        data['open_price'],
                        data['high_price'],
                        data['low_price'],
                        data['close_price'],
                        data['price_change'],
                        data['percentage_change'],
                        data['volume']
                    ))
                
                # åŸ·è¡Œæ‰¹é‡æ’å…¥
                pg_cursor.executemany(insert_sql, values_list)
                pg_conn.commit()
                
                # æ¢å¾©è¨­å®š
                pg_cursor.execute("SET synchronous_commit = ON")
                
            logger.info("âœ… æ‰¹é‡æ’å…¥å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥å¤±æ•—: {e}")
            return False

    def _import_using_executemany_optimized(self, converted_data):
        """ä½¿ç”¨execute_valuesé€²è¡Œå„ªåŒ–åŒ¯å…¥"""
        try:
            from psycopg2.extras import execute_values
            logger.info("âš¡ ä½¿ç”¨execute_valuesé€²è¡Œå„ªåŒ–åŒ¯å…¥...")
            
            with shared.get_conn_cur_from_pool_b(as_dict=False) as (pg_conn, pg_cursor):
                # æ€§èƒ½å„ªåŒ–è¨­å®š
                pg_cursor.execute("SET synchronous_commit = OFF")
                
                values_list = []
                for data in converted_data:
                    values_list.append((
                        data['trade_datetime'],
                        data['open_price'],
                        data['high_price'],
                        data['low_price'],
                        data['close_price'],
                        data['price_change'],
                        data['percentage_change'],
                        data['volume']
                    ))
                
                # ä½¿ç”¨execute_valuesé€²è¡Œé«˜æ•ˆæ’å…¥
                execute_values(
                    pg_cursor,
                    """
                    INSERT INTO stock_prices (
                        trade_datetime, open_price, high_price, low_price,
                        close_price, price_change, percentage_change, volume
                    ) VALUES %s
                    ON CONFLICT (trade_datetime) DO NOTHING
                    """,
                    values_list,
                    page_size=1000
                )
                
                pg_conn.commit()
                pg_cursor.execute("SET synchronous_commit = ON")
                
            logger.info("âœ… execute_valuesåŒ¯å…¥å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ execute_valuesåŒ¯å…¥å¤±æ•—: {e}")
            return False

    def benchmark_import_methods(self, symbol='MTX00', kline_type='MINUTE'):
        """æ¸¬è©¦ä¸åŒåŒ¯å…¥æ–¹æ³•çš„æ€§èƒ½"""
        logger.info("ğŸ é–‹å§‹æ€§èƒ½æ¸¬è©¦...")
        
        methods = ['copy', 'executemany', 'batch']
        results = {}
        
        for method in methods:
            logger.info(f"ğŸ§ª æ¸¬è©¦æ–¹æ³•: {method}")
            start_time = time.time()
            
            success = self.import_kline_to_postgres_fast(symbol, kline_type, method)
            
            elapsed_time = time.time() - start_time
            results[method] = {
                'success': success,
                'time': elapsed_time
            }
            
            logger.info(f"ğŸ“Š {method} æ–¹æ³•: {'æˆåŠŸ' if success else 'å¤±æ•—'} (è€—æ™‚: {elapsed_time:.2f}ç§’)")
        
        # é¡¯ç¤ºçµæœ
        logger.info("ğŸ† æ€§èƒ½æ¸¬è©¦çµæœ:")
        for method, result in results.items():
            if result['success']:
                logger.info(f"  {method}: {result['time']:.2f}ç§’")
        
        return results
