#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PostgreSQLåŒ¯å…¥å™¨ - å°‡SQLiteçš„Kç·šè³‡æ–™åŒ¯å…¥åˆ°PostgreSQLçš„stock_priceè¡¨
åŸºæ–¼æ‚¨ç¾æœ‰çš„è³‡æ–™åº«é€£æ¥æ–¹å¼
"""

import os
import sys
import logging
import sqlite3
from datetime import datetime
from decimal import Decimal

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å°å…¥æ‚¨çš„è³‡æ–™åº«æ¨¡çµ„
try:
    from app_setup import init_all_db_pools
    import shared
    HAS_POSTGRES_MODULES = True
except ImportError as e:
    HAS_POSTGRES_MODULES = False
    print(f"âš ï¸ ç„¡æ³•å°å…¥PostgreSQLæ¨¡çµ„: {e}")
    print("è«‹ç¢ºèªapp_setup.pyå’Œshared.pyåœ¨æ­£ç¢ºçš„è·¯å¾‘ä¸­")

from history_config import DATABASE_PATH

logger = logging.getLogger(__name__)

class PostgreSQLImporter:
    """PostgreSQLåŒ¯å…¥å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–åŒ¯å…¥å™¨"""
        self.sqlite_db_path = "data/history_data.db"
        self.postgres_initialized = False
        
        if not HAS_POSTGRES_MODULES:
            logger.error("âŒ PostgreSQLæ¨¡çµ„æœªæ‰¾åˆ°ï¼Œç„¡æ³•é€²è¡ŒåŒ¯å…¥")
            return
        
        # åˆå§‹åŒ–PostgreSQLé€£ç·šæ± 
        try:
            init_all_db_pools()
            self.postgres_initialized = True
            logger.info("âœ… PostgreSQLé€£ç·šæ± åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ PostgreSQLé€£ç·šæ± åˆå§‹åŒ–å¤±æ•—: {e}")

    def check_sqlite_data(self):
        """æª¢æŸ¥SQLiteè³‡æ–™åº«ä¸­çš„Kç·šè³‡æ–™"""
        try:
            if not os.path.exists(self.sqlite_db_path):
                logger.error(f"âŒ SQLiteè³‡æ–™åº«æª”æ¡ˆä¸å­˜åœ¨: {self.sqlite_db_path}")
                return None

            conn = sqlite3.connect(self.sqlite_db_path)
            conn.row_factory = sqlite3.Row
            
            # çµ±è¨ˆKç·šè³‡æ–™
            cursor = conn.execute("""
                SELECT 
                    symbol,
                    kline_type,
                    COUNT(*) as count,
                    MIN(trade_date || ' ' || COALESCE(trade_time, '00:00:00')) as min_datetime,
                    MAX(trade_date || ' ' || COALESCE(trade_time, '00:00:00')) as max_datetime
                FROM kline_data 
                GROUP BY symbol, kline_type
                ORDER BY symbol, kline_type
            """)
            
            results = cursor.fetchall()
            conn.close()
            
            if not results:
                logger.warning("âš ï¸ SQLiteè³‡æ–™åº«ä¸­æ²’æœ‰Kç·šè³‡æ–™")
                return None
            
            logger.info("ğŸ“Š SQLite Kç·šè³‡æ–™çµ±è¨ˆ:")
            for row in results:
                logger.info(f"  {row['symbol']} {row['kline_type']}: {row['count']} ç­†")
                logger.info(f"    æ™‚é–“ç¯„åœ: {row['min_datetime']} ~ {row['max_datetime']}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æª¢æŸ¥SQLiteè³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def convert_kline_to_stock_price_format(self, kline_data, exclude_anomalies=True):
        """
        å°‡Kç·šè³‡æ–™è½‰æ›ç‚ºstock_priceè¡¨æ ¼å¼

        Args:
            kline_data: Kç·šè³‡æ–™å­—å…¸
            exclude_anomalies: æ˜¯å¦æ’é™¤ç•°å¸¸è³‡æ–™ (é è¨­: True)
        """
        try:
            # è§£ææ—¥æœŸæ™‚é–“
            # æ ¹æ“šå¯¦éš›è³‡æ–™æ ¼å¼ï¼Œtrade_dateåŒ…å«å®Œæ•´çš„æ—¥æœŸæ™‚é–“ï¼ˆå¦‚ "2025/06/05 08:46"ï¼‰
            # trade_time æ˜¯ None
            date_str = str(kline_data['trade_date']).strip()  # "2025/06/05 08:46"

            # æ·»åŠ èª¿è©¦è³‡è¨Šï¼ˆåªåœ¨å‰å¹¾ç­†è³‡æ–™æ™‚é¡¯ç¤ºï¼‰
            if hasattr(self, '_debug_count'):
                self._debug_count += 1
            else:
                self._debug_count = 1

            if self._debug_count <= 3:
                logger.info(f"ğŸ” è½‰æ›ç¬¬{self._debug_count}ç­†è³‡æ–™: trade_date='{date_str}', trade_time='{kline_data.get('trade_time')}'")

            # ç›´æ¥è§£ææ—¥æœŸæ™‚é–“å­—ä¸²
            # é æœŸæ ¼å¼ï¼šYYYY/MM/DD HH:MM
            try:
                # å˜—è©¦è§£æå®Œæ•´çš„æ—¥æœŸæ™‚é–“æ ¼å¼
                trade_datetime = datetime.strptime(date_str, '%Y/%m/%d %H:%M')
            except ValueError:
                # å¦‚æœè§£æå¤±æ•—ï¼Œå˜—è©¦å…¶ä»–æ ¼å¼
                try:
                    # å˜—è©¦åªæœ‰æ—¥æœŸçš„æ ¼å¼
                    trade_datetime = datetime.strptime(date_str, '%Y/%m/%d')
                    # è¨­ç‚ºæ”¶ç›¤æ™‚é–“
                    trade_datetime = trade_datetime.replace(hour=13, minute=45, second=0)
                except ValueError:
                    # å¦‚æœé‚„æ˜¯å¤±æ•—ï¼Œè¨˜éŒ„éŒ¯èª¤ä¸¦è¿”å›None
                    logger.error(f"âŒ ç„¡æ³•è§£ææ—¥æœŸæ ¼å¼: '{date_str}'")
                    return None

            # è¨ˆç®—åƒ¹æ ¼è®ŠåŒ–ï¼ˆé€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è©²èˆ‡å‰ä¸€ç­†æ¯”è¼ƒï¼‰
            price_change = Decimal('0.00')  # æš«æ™‚è¨­ç‚º0ï¼Œå¯ä»¥å¾ŒçºŒå„ªåŒ–

            # è½‰æ›åƒ¹æ ¼è³‡æ–™
            open_price = Decimal(str(kline_data['open_price']))
            high_price = Decimal(str(kline_data['high_price']))
            low_price = Decimal(str(kline_data['low_price']))
            close_price = Decimal(str(kline_data['close_price']))
            volume = kline_data['volume'] or 0

            # è³‡æ–™é©—è­‰å’Œç•°å¸¸æª¢æ¸¬
            anomaly_detected = False

            # æª¢æŸ¥æ‰€æœ‰åƒ¹æ ¼æ˜¯å¦ç›¸åŒ
            if open_price == high_price == low_price == close_price:
                logger.warning(f"âš ï¸ ç™¼ç¾ç•°å¸¸è³‡æ–™ï¼šæ‰€æœ‰åƒ¹æ ¼éƒ½ç›¸åŒ {open_price} at {trade_datetime}")
                anomaly_detected = True

            # æª¢æŸ¥æˆäº¤é‡æ˜¯å¦ç‚º0
            if volume == 0:
                logger.warning(f"âš ï¸ ç™¼ç¾ç•°å¸¸è³‡æ–™ï¼šæˆäº¤é‡ç‚º0 at {trade_datetime}")
                anomaly_detected = True

            # åƒ¹æ ¼åˆç†æ€§æª¢æŸ¥ï¼ˆé€™æ˜¯åš´é‡éŒ¯èª¤ï¼Œå¿…é ˆæ’é™¤ï¼‰
            if high_price < max(open_price, close_price) or low_price > min(open_price, close_price):
                logger.error(f"âŒ åƒ¹æ ¼é‚è¼¯éŒ¯èª¤ at {trade_datetime}: O:{open_price} H:{high_price} L:{low_price} C:{close_price}")
                return None

            # æ ¹æ“šè¨­å®šæ±ºå®šæ˜¯å¦æ’é™¤ç•°å¸¸è³‡æ–™
            if exclude_anomalies and anomaly_detected:
                logger.info(f"ğŸš« æ’é™¤ç•°å¸¸è³‡æ–™ at {trade_datetime}")
                return None

            converted_data = {
                'trade_datetime': trade_datetime,
                'open_price': open_price,
                'high_price': high_price,
                'low_price': low_price,
                'close_price': close_price,
                'price_change': price_change,
                'percentage_change': Decimal('0.0000'),  # å¿½ç•¥æ­¤æ¬„ä½
                'volume': volume
            }

            # åˆ—å°å‰10è¡Œè½‰æ›å¾Œçš„è³‡æ–™åˆ°console
            if self._debug_count <= 10:
                print(f"\n=== PostgreSQLåŒ¯å…¥ - ç¬¬ {self._debug_count} ç­†è½‰æ›å¾Œçš„è³‡æ–™ ===")
                print(f"åŸå§‹Kç·šè³‡æ–™:")
                print(f"  å•†å“ä»£ç¢¼: {kline_data['symbol']}")
                print(f"  Kç·šé¡å‹: {kline_data['kline_type']}")
                print(f"  äº¤æ˜“æ—¥æœŸ: {kline_data['trade_date']}")
                print(f"  äº¤æ˜“æ™‚é–“: {kline_data['trade_time']}")
                print(f"  é–‹ç›¤åƒ¹: {kline_data['open_price']}")
                print(f"  æœ€é«˜åƒ¹: {kline_data['high_price']}")
                print(f"  æœ€ä½åƒ¹: {kline_data['low_price']}")
                print(f"  æ”¶ç›¤åƒ¹: {kline_data['close_price']}")
                print(f"  æˆäº¤é‡: {kline_data['volume']}")
                print(f"è½‰æ›ç‚ºPostgreSQLæ ¼å¼:")
                print(f"  äº¤æ˜“æ™‚é–“: {converted_data['trade_datetime']}")
                print(f"  é–‹ç›¤åƒ¹: {converted_data['open_price']}")
                print(f"  æœ€é«˜åƒ¹: {converted_data['high_price']}")
                print(f"  æœ€ä½åƒ¹: {converted_data['low_price']}")
                print(f"  æ”¶ç›¤åƒ¹: {converted_data['close_price']}")
                print(f"  åƒ¹æ ¼è®ŠåŒ–: {converted_data['price_change']}")
                print(f"  ç™¾åˆ†æ¯”è®ŠåŒ–: {converted_data['percentage_change']}")
                print(f"  æˆäº¤é‡: {converted_data['volume']}")
                print("=" * 60)

            return converted_data

        except Exception as e:
            logger.error(f"âŒ è½‰æ›è³‡æ–™æ ¼å¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            logger.error(f"   åŸå§‹è³‡æ–™: {kline_data}")
            logger.error(f"   æ—¥æœŸå­—ä¸²: '{date_str}'")
            return None

    def import_kline_to_postgres(self, symbol='MTX00', kline_type='MINUTE', batch_size=5000, use_copy=False, optimize_performance=True, exclude_anomalies=True):
        """åŒ¯å…¥Kç·šè³‡æ–™åˆ°PostgreSQL"""
        if not self.postgres_initialized:
            logger.error("âŒ PostgreSQLæœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŒ¯å…¥")
            return False

        # å¦‚æœä½¿ç”¨COPYæ–¹å¼ï¼ˆè¶…é«˜é€Ÿï¼‰
        if use_copy:
            return self._import_using_copy(symbol, kline_type)

        try:
            import time
            total_start_time = time.time()

            logger.info(f"ğŸš€ é–‹å§‹åŒ¯å…¥ {symbol} {kline_type} Kç·šè³‡æ–™åˆ°PostgreSQL...")
            logger.info(f"âš¡ æ€§èƒ½å„ªåŒ–æ¨¡å¼: {'é–‹å•Ÿ' if optimize_performance else 'é—œé–‰'}")
            logger.info(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")

            # é€£æ¥SQLiteè®€å–è³‡æ–™
            sqlite_conn = sqlite3.connect(self.sqlite_db_path)
            sqlite_conn.row_factory = sqlite3.Row

            # æŸ¥è©¢Kç·šè³‡æ–™
            query_sql = """
                SELECT * FROM kline_data
                WHERE symbol = ? AND kline_type = ?
                ORDER BY trade_date, trade_time
            """
            logger.info(f"ğŸ” åŸ·è¡ŒSQLiteæŸ¥è©¢: {query_sql.strip()}")
            logger.info(f"ğŸ” æŸ¥è©¢åƒæ•¸: symbol='{symbol}', kline_type='{kline_type}'")

            sqlite_cursor = sqlite_conn.execute(query_sql, (symbol, kline_type))

            # æª¢æŸ¥æŸ¥è©¢çµæœ
            all_rows = sqlite_cursor.fetchall()
            logger.info(f"ğŸ“Š å¾SQLiteæŸ¥è©¢åˆ° {len(all_rows)} ç­† {symbol} {kline_type} è³‡æ–™")

            # å¦‚æœæ²’æœ‰è³‡æ–™ï¼Œæª¢æŸ¥è³‡æ–™åº«ä¸­å¯¦éš›æœ‰ä»€éº¼
            if len(all_rows) == 0:
                logger.warning(f"âš ï¸ æ²’æœ‰æ‰¾åˆ° {symbol} {kline_type} è³‡æ–™ï¼Œæª¢æŸ¥è³‡æ–™åº«ä¸­çš„å¯¦éš›è³‡æ–™...")
                check_cursor = sqlite_conn.execute("""
                    SELECT symbol, kline_type, COUNT(*) as count
                    FROM kline_data
                    GROUP BY symbol, kline_type
                """)
                existing_data = check_cursor.fetchall()
                logger.info("ğŸ“Š è³‡æ–™åº«ä¸­ç¾æœ‰çš„è³‡æ–™:")
                for row in existing_data:
                    logger.info(f"  - {row['symbol']} {row['kline_type']}: {row['count']} ç­†")
                return False
            
            # ä½¿ç”¨æ‚¨çš„PostgreSQLé€£æ¥æ–¹å¼
            with shared.get_conn_cur_from_pool_b(as_dict=False) as (pg_conn, pg_cursor):

                # æ€§èƒ½å„ªåŒ–è¨­å®š
                if optimize_performance:
                    logger.info("âš¡ å•Ÿç”¨æ€§èƒ½å„ªåŒ–è¨­å®š...")
                    try:
                        # æš«æ™‚é—œé–‰åŒæ­¥æäº¤ä»¥æå‡æ€§èƒ½
                        pg_cursor.execute("SET synchronous_commit = OFF")
                        logger.info("âœ… å·²é—œé–‰åŒæ­¥æäº¤")

                        # å¢åŠ å·¥ä½œè¨˜æ†¶é«”
                        pg_cursor.execute("SET work_mem = '256MB'")
                        logger.info("âœ… å·²å¢åŠ å·¥ä½œè¨˜æ†¶é«”")

                    except Exception as e:
                        logger.warning(f"âš ï¸ éƒ¨åˆ†æ€§èƒ½è¨­å®šå¤±æ•— (å¯å¿½ç•¥): {e}")

                # æª¢æŸ¥stock_pricesè¡¨æ˜¯å¦å­˜åœ¨
                pg_cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables
                        WHERE table_name = 'stock_prices'
                    )
                """)

                if not pg_cursor.fetchone()[0]:
                    logger.error("âŒ PostgreSQLä¸­æ‰¾ä¸åˆ°stock_pricesè¡¨")
                    return False
                
                # å„ªåŒ–: é å…ˆè½‰æ›æ‰€æœ‰è³‡æ–™
                logger.info("ğŸ”„ é å…ˆè½‰æ›æ‰€æœ‰è³‡æ–™...")
                conversion_start = time.time()
                converted_data = []
                conversion_errors = 0

                for row in all_rows:
                    stock_price_data = self.convert_kline_to_stock_price_format(dict(row), exclude_anomalies)
                    if stock_price_data is None:
                        conversion_errors += 1
                        continue
                    converted_data.append(stock_price_data)

                conversion_time = time.time() - conversion_start
                logger.info(f"âœ… è³‡æ–™è½‰æ›å®Œæˆ (è€—æ™‚: {conversion_time:.2f}ç§’)")
                logger.info(f"ğŸ“Š æˆåŠŸè½‰æ›: {len(converted_data)} ç­†ï¼Œæ’é™¤ç•°å¸¸/éŒ¯èª¤: {conversion_errors} ç­†")

                if exclude_anomalies and conversion_errors > 0:
                    logger.info(f"ğŸš« ç•°å¸¸è³‡æ–™æ’é™¤è¨­å®š: {'é–‹å•Ÿ' if exclude_anomalies else 'é—œé–‰'}")
                    logger.info(f"   æ’é™¤çš„è³‡æ–™åŒ…å«ï¼šæ‰€æœ‰åƒ¹æ ¼ç›¸åŒã€æˆäº¤é‡ç‚º0ã€åƒ¹æ ¼é‚è¼¯éŒ¯èª¤ç­‰")

                if len(converted_data) == 0:
                    logger.warning("âš ï¸ æ²’æœ‰æœ‰æ•ˆè³‡æ–™å¯åŒ¯å…¥")
                    return False

                # æ‰¹é‡è™•ç†è³‡æ–™
                total_inserted = 0
                batch_count = 0

                # åˆ†æ‰¹è™•ç†
                for i in range(0, len(converted_data), batch_size):
                    batch_data = converted_data[i:i + batch_size]
                    batch_count += 1

                    batch_start = time.time()
                    inserted_count = self._insert_batch_to_postgres(pg_cursor, batch_data)
                    batch_time = time.time() - batch_start

                    total_inserted += inserted_count

                    # æ¯å€‹æ‰¹æ¬¡æäº¤
                    pg_conn.commit()

                    # æ¸›å°‘æ—¥èªŒè¼¸å‡ºé »ç‡
                    if batch_count % 5 == 0 or batch_count == 1:
                        logger.info(f"ğŸ“¦ æ‰¹æ¬¡ {batch_count}: {inserted_count}/{len(batch_data)} ç­† (è€—æ™‚: {batch_time:.2f}ç§’)")

                    # æ¯10å€‹æ‰¹æ¬¡é¡¯ç¤ºé€²åº¦
                    if batch_count % 10 == 0:
                        progress = (i + len(batch_data)) / len(converted_data) * 100
                        logger.info(f"ğŸ“Š é€²åº¦: {progress:.1f}% ({i + len(batch_data)}/{len(converted_data)})")

                # æ¢å¾©æ­£å¸¸è¨­å®š
                if optimize_performance:
                    try:
                        pg_cursor.execute("SET synchronous_commit = ON")
                        logger.info("âœ… å·²æ¢å¾©åŒæ­¥æäº¤è¨­å®š")
                    except:
                        pass

                total_time = time.time() - total_start_time
                logger.info(f"âœ… åŒ¯å…¥å®Œæˆï¼")
                logger.info(f"ğŸ“Š çµ±è¨ˆçµæœ:")
                logger.info(f"  - ç¸½è™•ç†ç­†æ•¸: {len(all_rows)}")
                logger.info(f"  - æˆåŠŸè½‰æ›: {len(converted_data)}")
                logger.info(f"  - æˆåŠŸæ’å…¥: {total_inserted}")
                logger.info(f"  - è½‰æ›éŒ¯èª¤: {conversion_errors}")
                logger.info(f"  - ç¸½è€—æ™‚: {total_time:.2f}ç§’")
                logger.info(f"  - å¹³å‡é€Ÿåº¦: {total_inserted/total_time:.0f} ç­†/ç§’")
                
            sqlite_conn.close()
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŒ¯å…¥éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return False

    def _insert_batch_to_postgres(self, cursor, batch_data):
        """æ‰¹é‡æ’å…¥è³‡æ–™åˆ°PostgreSQL - å„ªåŒ–ç‰ˆæœ¬"""
        if not batch_data:
            return 0

        import time
        start_time = time.time()

        try:
            # æ€§èƒ½å„ªåŒ–: æ¸›å°‘æ—¥èªŒè¼¸å‡º
            if len(batch_data) > 100:
                logger.info(f"ğŸ” æº–å‚™æ’å…¥ {len(batch_data)} ç­†è³‡æ–™åˆ°PostgreSQL")

            # å˜—è©¦ä½¿ç”¨ execute_values (æ›´é«˜æ•ˆ)
            try:
                from psycopg2.extras import execute_values

                # ä½¿ç”¨ execute_values é€²è¡Œé«˜æ•ˆæ‰¹é‡æ’å…¥
                insert_sql = """
                    INSERT INTO stock_prices (
                        trade_datetime, open_price, high_price, low_price,
                        close_price, price_change, percentage_change, volume
                    ) VALUES %s
                    ON CONFLICT (trade_datetime) DO NOTHING
                """

                values_list = []
                for data in batch_data:
                    values_list.append((
                        data['trade_datetime'],
                        data['open_price'],
                        data['high_price'],
                        data['low_price'],
                        data['close_price'],
                        data['price_change'],
                        data['percentage_change'],
                        data['volume']
                    ))

                # ä½¿ç”¨ execute_values é€²è¡Œé«˜æ•ˆæ’å…¥
                execute_values(cursor, insert_sql, values_list, page_size=1000)
                inserted_count = len(batch_data)  # execute_values ä¸è¿”å› rowcount

                total_time = time.time() - start_time
                if len(batch_data) > 100:
                    logger.info(f"âœ… execute_values æ’å…¥å®Œæˆ: {inserted_count} ç­† (è€—æ™‚: {total_time:.2f}ç§’)")

                return inserted_count

            except ImportError:
                # å¦‚æœæ²’æœ‰ execute_valuesï¼Œä½¿ç”¨æ¨™æº–æ–¹æ³•
                logger.warning("âš ï¸ execute_values ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨™æº– executemany")

                # æ¨™æº–æ‰¹é‡æ’å…¥æ–¹æ³•
                insert_sql = """
                    INSERT INTO stock_prices (
                        trade_datetime, open_price, high_price, low_price,
                        close_price, price_change, percentage_change, volume
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (trade_datetime) DO NOTHING
                """

            logger.info(f"ğŸ” ä½¿ç”¨å„ªåŒ–çš„ä¸»éµè¡çªè™•ç†SQL")

            # æº–å‚™è³‡æ–™ï¼ˆåªéœ€è¦8å€‹åƒæ•¸ï¼‰
            values_list = []
            for i, data in enumerate(batch_data):
                values = (
                    data['trade_datetime'],
                    data['open_price'],
                    data['high_price'],
                    data['low_price'],
                    data['close_price'],
                    data['price_change'],
                    data['percentage_change'],
                    data['volume']
                )
                values_list.append(values)

                # é¡¯ç¤ºå‰3ç­†è³‡æ–™çš„è©³ç´°è³‡è¨Š
                if i < 3:
                    logger.info(f"ğŸ” ç¬¬{i+1}ç­†è³‡æ–™: {data['trade_datetime']} OHLC:{data['open_price']}/{data['high_price']}/{data['low_price']}/{data['close_price']} V:{data['volume']}")

            logger.info(f"ğŸ” æº–å‚™åŸ·è¡Œæ‰¹é‡æ’å…¥ï¼Œå…± {len(values_list)} ç­†è³‡æ–™")

            try:
                # ä½¿ç”¨ä¸»éµç´„æŸçš„ON CONFLICTè™•ç†
                exec_start = time.time()
                logger.info("ğŸ” é–‹å§‹åŸ·è¡Œ cursor.executemany()...")
                cursor.executemany(insert_sql, values_list)
                exec_time = time.time() - exec_start
                logger.info(f"ğŸ” executemany() åŸ·è¡Œå®Œæˆ (è€—æ™‚: {exec_time:.2f}ç§’)")

                inserted_count = cursor.rowcount
                logger.info(f"ğŸ” cursor.rowcount = {inserted_count}")

                # æª¢æŸ¥æ˜¯å¦çœŸçš„æ’å…¥äº†è³‡æ–™
                cursor.execute("SELECT COUNT(*) FROM stock_prices WHERE trade_datetime >= %s", (values_list[0][0],))
                actual_count = cursor.fetchone()[0]
                logger.info(f"ğŸ” è³‡æ–™åº«ä¸­å¯¦éš›ç­†æ•¸æª¢æŸ¥: {actual_count}")

                total_time = time.time() - start_time
                logger.info(f"ğŸ” PostgreSQLå›å ±ï¼šæˆåŠŸæ’å…¥ {inserted_count} ç­†è³‡æ–™")
                logger.info(f"â±ï¸ æ‰¹æ¬¡ç¸½è€—æ™‚: {total_time:.2f}ç§’ (å¹³å‡: {total_time/len(batch_data)*1000:.1f}ms/ç­†)")

                return inserted_count

            except Exception as e:
                logger.error(f"âŒ åŸ·è¡Œæ‰¹é‡æ’å…¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                logger.error(f"âŒ éŒ¯èª¤é¡å‹: {type(e).__name__}")
                import traceback
                logger.error(f"âŒ è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
                return 0
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return 0

    def check_postgres_data(self):
        """æª¢æŸ¥PostgreSQLä¸­çš„stock_priceè³‡æ–™"""
        if not self.postgres_initialized:
            logger.error("âŒ PostgreSQLæœªåˆå§‹åŒ–")
            return False
        
        try:
            with shared.get_conn_cur_from_pool_b(as_dict=True) as (conn, cursor):
                # çµ±è¨ˆè³‡æ–™
                cursor.execute("""
                    SELECT
                        COUNT(*) as total_count,
                        MIN(trade_datetime) as min_datetime,
                        MAX(trade_datetime) as max_datetime,
                        COUNT(DISTINCT trade_datetime::date) as trading_days
                    FROM stock_prices
                """)
                
                result = cursor.fetchone()
                
                if result['total_count'] > 0:
                    logger.info("ğŸ“Š PostgreSQL stock_pricesè¡¨çµ±è¨ˆ:")
                    logger.info(f"  - ç¸½ç­†æ•¸: {result['total_count']:,}")
                    logger.info(f"  - äº¤æ˜“æ—¥æ•¸: {result['trading_days']}")
                    logger.info(f"  - æ™‚é–“ç¯„åœ: {result['min_datetime']} ~ {result['max_datetime']}")

                    # é¡¯ç¤ºæœ€æ–°å¹¾ç­†è³‡æ–™
                    cursor.execute("""
                        SELECT trade_datetime, open_price, high_price, low_price, close_price, volume
                        FROM stock_prices
                        ORDER BY trade_datetime DESC
                        LIMIT 5
                    """)
                    
                    recent_data = cursor.fetchall()
                    logger.info("ğŸ“ˆ æœ€æ–°5ç­†è³‡æ–™:")
                    for row in recent_data:
                        logger.info(f"  {row['trade_datetime']} O:{row['open_price']} H:{row['high_price']} L:{row['low_price']} C:{row['close_price']} V:{row['volume']}")
                else:
                    logger.warning("âš ï¸ PostgreSQL stock_pricesè¡¨ä¸­æ²’æœ‰è³‡æ–™")
                
                return True
                
        except Exception as e:
            logger.error(f"âŒ æª¢æŸ¥PostgreSQLè³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def _import_using_copy(self, symbol, kline_type):
        """ä½¿ç”¨COPYå‘½ä»¤è¶…é«˜é€ŸåŒ¯å…¥ï¼ˆé©åˆå¤§é‡è³‡æ–™ï¼‰"""
        import tempfile
        import csv
        import time

        try:
            logger.info(f"ğŸš€ ä½¿ç”¨COPYæ–¹å¼åŒ¯å…¥ {symbol} {kline_type} è³‡æ–™...")
            start_time = time.time()

            # é€£æ¥SQLiteè®€å–è³‡æ–™
            sqlite_conn = sqlite3.connect(self.sqlite_db_path)
            sqlite_conn.row_factory = sqlite3.Row

            # æŸ¥è©¢Kç·šè³‡æ–™
            sqlite_cursor = sqlite_conn.execute("""
                SELECT * FROM kline_data
                WHERE symbol = ? AND kline_type = ?
                ORDER BY trade_date, trade_time
            """, (symbol, kline_type))

            all_rows = sqlite_cursor.fetchall()
            logger.info(f"ğŸ“Š æŸ¥è©¢åˆ° {len(all_rows)} ç­†è³‡æ–™")

            if len(all_rows) == 0:
                logger.warning("âš ï¸ æ²’æœ‰è³‡æ–™å¯åŒ¯å…¥")
                return False

            # å‰µå»ºè‡¨æ™‚CSVæª”æ¡ˆ
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8') as temp_file:
                csv_writer = csv.writer(temp_file)

                converted_count = 0
                for row in all_rows:
                    # è½‰æ›è³‡æ–™æ ¼å¼
                    stock_price_data = self.convert_kline_to_stock_price_format(dict(row))
                    if stock_price_data:
                        csv_writer.writerow([
                            stock_price_data['trade_datetime'],
                            stock_price_data['open_price'],
                            stock_price_data['high_price'],
                            stock_price_data['low_price'],
                            stock_price_data['close_price'],
                            stock_price_data['price_change'],
                            stock_price_data['percentage_change'],
                            stock_price_data['volume']
                        ])
                        converted_count += 1

                temp_file_path = temp_file.name

            logger.info(f"ğŸ“ å·²è½‰æ› {converted_count} ç­†è³‡æ–™åˆ°è‡¨æ™‚æª”æ¡ˆ: {temp_file_path}")

            # ä½¿ç”¨COPYå‘½ä»¤åŒ¯å…¥
            with shared.get_conn_cur_from_pool_b(as_dict=False) as (pg_conn, pg_cursor):
                copy_start = time.time()

                with open(temp_file_path, 'r', encoding='utf-8') as f:
                    pg_cursor.copy_expert("""
                        COPY stock_prices (
                            trade_datetime, open_price, high_price, low_price,
                            close_price, price_change, percentage_change, volume
                        ) FROM STDIN WITH CSV
                    """, f)

                pg_conn.commit()
                copy_time = time.time() - copy_start

                logger.info(f"âš¡ COPYå‘½ä»¤åŸ·è¡Œå®Œæˆ (è€—æ™‚: {copy_time:.2f}ç§’)")

                # æª¢æŸ¥æ’å…¥çµæœ
                pg_cursor.execute("SELECT COUNT(*) FROM stock_prices")
                total_count = pg_cursor.fetchone()[0]

                total_time = time.time() - start_time
                logger.info(f"âœ… COPYåŒ¯å…¥å®Œæˆï¼")
                logger.info(f"ğŸ“Š ç¸½è€—æ™‚: {total_time:.2f}ç§’")
                logger.info(f"ğŸ“Š å¹³å‡é€Ÿåº¦: {converted_count/total_time:.0f} ç­†/ç§’")
                logger.info(f"ğŸ“Š è³‡æ–™åº«ç¸½ç­†æ•¸: {total_count:,}")

            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            import os
            os.unlink(temp_file_path)
            sqlite_conn.close()

            return True

        except Exception as e:
            logger.error(f"âŒ COPYåŒ¯å…¥å¤±æ•—: {e}")
            import traceback
            logger.error(f"âŒ è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            return False

    def convert_tick_to_postgres_format(self, tick_data):
        """å°‡é€ç­†è³‡æ–™è½‰æ›ç‚ºPostgreSQLæ ¼å¼"""
        try:
            # è§£ææ—¥æœŸæ™‚é–“
            date_str = str(tick_data['trade_date']).strip()  # "20241201"
            time_str = str(tick_data['trade_time']).strip().zfill(6)  # "090000"

            # è½‰æ›ç‚º datetime
            trade_datetime = datetime.strptime(f"{date_str} {time_str}", '%Y%m%d %H%M%S')

            # è™•ç†æ¯«ç§’
            if tick_data.get('trade_time_ms'):
                microseconds = min(tick_data['trade_time_ms'] * 1000, 999999)
                trade_datetime = trade_datetime.replace(microsecond=microseconds)

            # è½‰æ›åƒ¹æ ¼ç‚ºDecimal
            bid_price = None
            ask_price = None
            close_price = Decimal(str(tick_data['close_price'])).quantize(Decimal('0.01'))

            if tick_data.get('bid_price') is not None:
                bid_price = Decimal(str(tick_data['bid_price'])).quantize(Decimal('0.01'))
            if tick_data.get('ask_price') is not None:
                ask_price = Decimal(str(tick_data['ask_price'])).quantize(Decimal('0.01'))

            converted_data = {
                'trade_datetime': trade_datetime,
                'symbol': tick_data['symbol'],
                'bid_price': bid_price,
                'ask_price': ask_price,
                'close_price': close_price,
                'volume': tick_data['volume'],
                'trade_time_ms': tick_data.get('trade_time_ms', 0),
                'market_no': tick_data.get('market_no', 0),
                'simulate_flag': tick_data.get('simulate_flag', 0)
            }

            # åˆ—å°å‰10è¡Œè½‰æ›å¾Œçš„è³‡æ–™åˆ°console
            if hasattr(self, '_tick_debug_count'):
                self._tick_debug_count += 1
            else:
                self._tick_debug_count = 1

            if self._tick_debug_count <= 10:
                print(f"\n=== Tick PostgreSQLåŒ¯å…¥ - ç¬¬ {self._tick_debug_count} ç­†è½‰æ›å¾Œçš„è³‡æ–™ ===")
                print(f"åŸå§‹é€ç­†è³‡æ–™:")
                print(f"  å•†å“ä»£ç¢¼: {tick_data['symbol']}")
                print(f"  äº¤æ˜“æ—¥æœŸ: {tick_data['trade_date']}")
                print(f"  äº¤æ˜“æ™‚é–“: {tick_data['trade_time']}")
                print(f"  è²·åƒ¹: {tick_data.get('bid_price')}")
                print(f"  è³£åƒ¹: {tick_data.get('ask_price')}")
                print(f"  æˆäº¤åƒ¹: {tick_data['close_price']}")
                print(f"  æˆäº¤é‡: {tick_data['volume']}")
                print(f"è½‰æ›ç‚ºPostgreSQLæ ¼å¼:")
                print(f"  äº¤æ˜“æ™‚é–“: {converted_data['trade_datetime']}")
                print(f"  å•†å“ä»£ç¢¼: {converted_data['symbol']}")
                print(f"  è²·åƒ¹: {converted_data['bid_price']}")
                print(f"  è³£åƒ¹: {converted_data['ask_price']}")
                print(f"  æˆäº¤åƒ¹: {converted_data['close_price']}")
                print(f"  æˆäº¤é‡: {converted_data['volume']}")
                print(f"  æ¯«ç§’: {converted_data['trade_time_ms']}")
                print("=" * 60)

            return converted_data

        except Exception as e:
            logger.error(f"âŒ è½‰æ›é€ç­†è³‡æ–™æ ¼å¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            logger.error(f"   åŸå§‹è³‡æ–™: {tick_data}")
            return None

    def _insert_tick_batch_to_postgres(self, cursor, batch_data, optimize_performance=True):
        """æ‰¹é‡æ’å…¥é€ç­†è³‡æ–™åˆ°PostgreSQL"""
        if not batch_data:
            return 0

        import time
        start_time = time.time()

        try:
            # å˜—è©¦ä½¿ç”¨ execute_values (æ›´é«˜æ•ˆ)
            try:
                from psycopg2.extras import execute_values

                insert_sql = """
                    INSERT INTO tick_prices (
                        trade_datetime, symbol, bid_price, ask_price, close_price,
                        volume, trade_time_ms, market_no, simulate_flag
                    ) VALUES %s
                    ON CONFLICT (trade_datetime, symbol) DO NOTHING
                """

                values_list = []
                for data in batch_data:
                    values_list.append((
                        data['trade_datetime'],
                        data['symbol'],
                        data['bid_price'],
                        data['ask_price'],
                        data['close_price'],
                        data['volume'],
                        data['trade_time_ms'],
                        data['market_no'],
                        data['simulate_flag']
                    ))

                # ä½¿ç”¨ execute_values é€²è¡Œé«˜æ•ˆæ’å…¥
                execute_values(cursor, insert_sql, values_list, page_size=1000)
                inserted_count = len(batch_data)

                total_time = time.time() - start_time
                if len(batch_data) > 100:
                    logger.info(f"âœ… é€ç­† execute_values æ’å…¥å®Œæˆ: {inserted_count} ç­† (è€—æ™‚: {total_time:.2f}ç§’)")

                return inserted_count

            except ImportError:
                logger.warning("âš ï¸ execute_values ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨™æº– executemany")

                # æ¨™æº–æ‰¹é‡æ’å…¥æ–¹æ³•
                insert_sql = """
                    INSERT INTO tick_prices (
                        trade_datetime, symbol, bid_price, ask_price, close_price,
                        volume, trade_time_ms, market_no, simulate_flag
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (trade_datetime, symbol) DO NOTHING
                """

                values_list = []
                for data in batch_data:
                    values_list.append((
                        data['trade_datetime'],
                        data['symbol'],
                        data['bid_price'],
                        data['ask_price'],
                        data['close_price'],
                        data['volume'],
                        data['trade_time_ms'],
                        data['market_no'],
                        data['simulate_flag']
                    ))

                cursor.executemany(insert_sql, values_list)
                inserted_count = cursor.rowcount

                total_time = time.time() - start_time
                logger.info(f"âœ… é€ç­†æ¨™æº–æ’å…¥å®Œæˆ: {inserted_count} ç­† (è€—æ™‚: {total_time:.2f}ç§’)")

                return inserted_count

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥é€ç­†è³‡æ–™å¤±æ•—: {e}")
            return 0

    def import_tick_to_postgres(self, symbol='MTX00', batch_size=5000, optimize_performance=True):
        """åŒ¯å…¥é€ç­†è³‡æ–™åˆ°PostgreSQL"""
        if not self.postgres_initialized:
            logger.error("âŒ PostgreSQLæœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŒ¯å…¥")
            return False

        try:
            import time
            total_start_time = time.time()

            logger.info(f"ğŸš€ é–‹å§‹åŒ¯å…¥ {symbol} é€ç­†è³‡æ–™åˆ°PostgreSQL...")
            logger.info(f"âš¡ æ€§èƒ½å„ªåŒ–æ¨¡å¼: {'é–‹å•Ÿ' if optimize_performance else 'é—œé–‰'}")
            logger.info(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")

            # é€£æ¥SQLiteè®€å–è³‡æ–™
            sqlite_conn = sqlite3.connect(self.sqlite_db_path)
            sqlite_conn.row_factory = sqlite3.Row

            # æŸ¥è©¢é€ç­†è³‡æ–™
            query_sql = """
                SELECT * FROM tick_data
                WHERE symbol = ?
                ORDER BY trade_date, trade_time
            """
            logger.info(f"ğŸ” åŸ·è¡ŒSQLiteæŸ¥è©¢: {query_sql.strip()}")
            logger.info(f"ğŸ” æŸ¥è©¢åƒæ•¸: symbol='{symbol}'")

            sqlite_cursor = sqlite_conn.execute(query_sql, (symbol,))
            all_rows = sqlite_cursor.fetchall()
            logger.info(f"ğŸ“Š å¾SQLiteæŸ¥è©¢åˆ° {len(all_rows)} ç­† {symbol} é€ç­†è³‡æ–™")

            if len(all_rows) == 0:
                logger.warning(f"âš ï¸ æ²’æœ‰æ‰¾åˆ° {symbol} é€ç­†è³‡æ–™")
                sqlite_conn.close()
                return False

            # ä½¿ç”¨PostgreSQLé€£æ¥
            with shared.get_conn_cur_from_pool_b(as_dict=False) as (pg_conn, pg_cursor):

                # æ€§èƒ½å„ªåŒ–è¨­å®š
                if optimize_performance:
                    logger.info("âš¡ å•Ÿç”¨æ€§èƒ½å„ªåŒ–è¨­å®š...")
                    try:
                        pg_cursor.execute("SET synchronous_commit = OFF")
                        pg_cursor.execute("SET work_mem = '256MB'")
                        logger.info("âœ… æ€§èƒ½å„ªåŒ–è¨­å®šå®Œæˆ")
                    except Exception as e:
                        logger.warning(f"âš ï¸ éƒ¨åˆ†æ€§èƒ½è¨­å®šå¤±æ•— (å¯å¿½ç•¥): {e}")

                # æª¢æŸ¥tick_pricesè¡¨æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»º
                pg_cursor.execute("""
                    CREATE TABLE IF NOT EXISTS tick_prices (
                        trade_datetime timestamp without time zone NOT NULL,
                        symbol varchar(20) NOT NULL,
                        bid_price numeric(10,2),
                        ask_price numeric(10,2),
                        close_price numeric(10,2) NOT NULL,
                        volume integer NOT NULL,
                        trade_time_ms integer,
                        market_no integer,
                        simulate_flag integer DEFAULT 0,
                        CONSTRAINT pk_tick_prices PRIMARY KEY (trade_datetime, symbol)
                    )
                """)
                logger.info("âœ… ç¢ºèªtick_pricesè¡¨å­˜åœ¨")

                # é å…ˆè½‰æ›æ‰€æœ‰è³‡æ–™
                logger.info("ğŸ”„ é å…ˆè½‰æ›æ‰€æœ‰é€ç­†è³‡æ–™...")
                conversion_start = time.time()
                converted_data = []
                conversion_errors = 0

                for row in all_rows:
                    tick_postgres_data = self.convert_tick_to_postgres_format(dict(row))
                    if tick_postgres_data is None:
                        conversion_errors += 1
                        continue
                    converted_data.append(tick_postgres_data)

                conversion_time = time.time() - conversion_start
                logger.info(f"âœ… é€ç­†è³‡æ–™è½‰æ›å®Œæˆ (è€—æ™‚: {conversion_time:.2f}ç§’)")
                logger.info(f"ğŸ“Š æˆåŠŸè½‰æ›: {len(converted_data)} ç­†ï¼ŒéŒ¯èª¤: {conversion_errors} ç­†")

                if len(converted_data) == 0:
                    logger.warning("âš ï¸ æ²’æœ‰æœ‰æ•ˆçš„é€ç­†è³‡æ–™å¯åŒ¯å…¥")
                    sqlite_conn.close()
                    return False

                # æ‰¹é‡è™•ç†è³‡æ–™
                total_inserted = 0
                batch_count = 0

                # åˆ†æ‰¹è™•ç†
                for i in range(0, len(converted_data), batch_size):
                    batch_data = converted_data[i:i + batch_size]
                    batch_count += 1

                    batch_start = time.time()
                    inserted_count = self._insert_tick_batch_to_postgres(pg_cursor, batch_data, optimize_performance)
                    batch_time = time.time() - batch_start

                    total_inserted += inserted_count

                    # æ¯å€‹æ‰¹æ¬¡æäº¤
                    pg_conn.commit()

                    # æ¸›å°‘æ—¥èªŒè¼¸å‡ºé »ç‡
                    if batch_count % 5 == 0 or batch_count == 1:
                        logger.info(f"ğŸ“¦ é€ç­†æ‰¹æ¬¡ {batch_count}: {inserted_count}/{len(batch_data)} ç­† (è€—æ™‚: {batch_time:.2f}ç§’)")

                # æ¢å¾©æ­£å¸¸è¨­å®š
                if optimize_performance:
                    try:
                        pg_cursor.execute("SET synchronous_commit = ON")
                        logger.info("âœ… å·²æ¢å¾©åŒæ­¥æäº¤è¨­å®š")
                    except:
                        pass

                total_time = time.time() - total_start_time
                logger.info(f"âœ… é€ç­†è³‡æ–™åŒ¯å…¥å®Œæˆï¼")
                logger.info(f"ğŸ“Š çµ±è¨ˆçµæœ:")
                logger.info(f"  - ç¸½è™•ç†ç­†æ•¸: {len(all_rows)}")
                logger.info(f"  - æˆåŠŸè½‰æ›: {len(converted_data)}")
                logger.info(f"  - æˆåŠŸæ’å…¥: {total_inserted}")
                logger.info(f"  - è½‰æ›éŒ¯èª¤: {conversion_errors}")
                logger.info(f"  - ç¸½è€—æ™‚: {total_time:.2f}ç§’")
                logger.info(f"  - å¹³å‡é€Ÿåº¦: {total_inserted/total_time:.0f} ç­†/ç§’")

            sqlite_conn.close()
            return True

        except Exception as e:
            logger.error(f"âŒ åŒ¯å…¥é€ç­†è³‡æ–™éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return False

    def convert_best5_to_postgres_format(self, best5_data):
        """å°‡äº”æª”è³‡æ–™è½‰æ›ç‚ºPostgreSQLæ ¼å¼"""
        try:
            # è§£ææ—¥æœŸæ™‚é–“
            date_str = str(best5_data['trade_date']).strip()  # "20241201"
            time_str = str(best5_data['trade_time']).strip().zfill(6)  # "090000"

            # è½‰æ›ç‚º datetime
            trade_datetime = datetime.strptime(f"{date_str} {time_str}", '%Y%m%d %H%M%S')

            # è½‰æ›åƒ¹æ ¼ç‚ºDecimalï¼Œè™•ç†Noneå€¼
            def convert_price(price):
                if price is None or price == 0:
                    return None
                return Decimal(str(price)).quantize(Decimal('0.01'))

            converted_data = {
                'trade_datetime': trade_datetime,
                'symbol': best5_data['symbol'],

                # äº”æª”è²·åƒ¹è²·é‡
                'bid_price_1': convert_price(best5_data.get('bid_price_1')),
                'bid_volume_1': best5_data.get('bid_volume_1', 0),
                'bid_price_2': convert_price(best5_data.get('bid_price_2')),
                'bid_volume_2': best5_data.get('bid_volume_2', 0),
                'bid_price_3': convert_price(best5_data.get('bid_price_3')),
                'bid_volume_3': best5_data.get('bid_volume_3', 0),
                'bid_price_4': convert_price(best5_data.get('bid_price_4')),
                'bid_volume_4': best5_data.get('bid_volume_4', 0),
                'bid_price_5': convert_price(best5_data.get('bid_price_5')),
                'bid_volume_5': best5_data.get('bid_volume_5', 0),

                # äº”æª”è³£åƒ¹è³£é‡
                'ask_price_1': convert_price(best5_data.get('ask_price_1')),
                'ask_volume_1': best5_data.get('ask_volume_1', 0),
                'ask_price_2': convert_price(best5_data.get('ask_price_2')),
                'ask_volume_2': best5_data.get('ask_volume_2', 0),
                'ask_price_3': convert_price(best5_data.get('ask_price_3')),
                'ask_volume_3': best5_data.get('ask_volume_3', 0),
                'ask_price_4': convert_price(best5_data.get('ask_price_4')),
                'ask_volume_4': best5_data.get('ask_volume_4', 0),
                'ask_price_5': convert_price(best5_data.get('ask_price_5')),
                'ask_volume_5': best5_data.get('ask_volume_5', 0),

                # å»¶ä¼¸è²·è³£
                'extend_bid': convert_price(best5_data.get('extend_bid')),
                'extend_bid_qty': best5_data.get('extend_bid_qty', 0),
                'extend_ask': convert_price(best5_data.get('extend_ask')),
                'extend_ask_qty': best5_data.get('extend_ask_qty', 0)
            }

            # åˆ—å°å‰10è¡Œè½‰æ›å¾Œçš„è³‡æ–™åˆ°console
            if hasattr(self, '_best5_debug_count'):
                self._best5_debug_count += 1
            else:
                self._best5_debug_count = 1

            if self._best5_debug_count <= 10:
                print(f"\n=== Best5 PostgreSQLåŒ¯å…¥ - ç¬¬ {self._best5_debug_count} ç­†è½‰æ›å¾Œçš„è³‡æ–™ ===")
                print(f"åŸå§‹äº”æª”è³‡æ–™:")
                print(f"  å•†å“ä»£ç¢¼: {best5_data['symbol']}")
                print(f"  äº¤æ˜“æ—¥æœŸ: {best5_data['trade_date']}")
                print(f"  äº¤æ˜“æ™‚é–“: {best5_data['trade_time']}")
                print(f"  è²·1: {best5_data.get('bid_price_1')} x {best5_data.get('bid_volume_1')}")
                print(f"  è²·2: {best5_data.get('bid_price_2')} x {best5_data.get('bid_volume_2')}")
                print(f"  è³£1: {best5_data.get('ask_price_1')} x {best5_data.get('ask_volume_1')}")
                print(f"  è³£2: {best5_data.get('ask_price_2')} x {best5_data.get('ask_volume_2')}")
                print(f"è½‰æ›ç‚ºPostgreSQLæ ¼å¼:")
                print(f"  äº¤æ˜“æ™‚é–“: {converted_data['trade_datetime']}")
                print(f"  å•†å“ä»£ç¢¼: {converted_data['symbol']}")
                print(f"  è²·1: {converted_data['bid_price_1']} x {converted_data['bid_volume_1']}")
                print(f"  è²·2: {converted_data['bid_price_2']} x {converted_data['bid_volume_2']}")
                print(f"  è³£1: {converted_data['ask_price_1']} x {converted_data['ask_volume_1']}")
                print(f"  è³£2: {converted_data['ask_price_2']} x {converted_data['ask_volume_2']}")
                print("=" * 60)

            return converted_data

        except Exception as e:
            logger.error(f"âŒ è½‰æ›äº”æª”è³‡æ–™æ ¼å¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            logger.error(f"   åŸå§‹è³‡æ–™: {best5_data}")
            return None

    def _insert_best5_batch_to_postgres(self, cursor, batch_data, optimize_performance=True):
        """æ‰¹é‡æ’å…¥äº”æª”è³‡æ–™åˆ°PostgreSQL"""
        if not batch_data:
            return 0

        import time
        start_time = time.time()

        try:
            # å˜—è©¦ä½¿ç”¨ execute_values (æ›´é«˜æ•ˆ)
            try:
                from psycopg2.extras import execute_values

                insert_sql = """
                    INSERT INTO best5_prices (
                        trade_datetime, symbol,
                        bid_price_1, bid_volume_1, bid_price_2, bid_volume_2,
                        bid_price_3, bid_volume_3, bid_price_4, bid_volume_4,
                        bid_price_5, bid_volume_5,
                        ask_price_1, ask_volume_1, ask_price_2, ask_volume_2,
                        ask_price_3, ask_volume_3, ask_price_4, ask_volume_4,
                        ask_price_5, ask_volume_5,
                        extend_bid, extend_bid_qty, extend_ask, extend_ask_qty
                    ) VALUES %s
                    ON CONFLICT (trade_datetime, symbol) DO NOTHING
                """

                values_list = []
                for data in batch_data:
                    values_list.append((
                        data['trade_datetime'],
                        data['symbol'],
                        data['bid_price_1'], data['bid_volume_1'],
                        data['bid_price_2'], data['bid_volume_2'],
                        data['bid_price_3'], data['bid_volume_3'],
                        data['bid_price_4'], data['bid_volume_4'],
                        data['bid_price_5'], data['bid_volume_5'],
                        data['ask_price_1'], data['ask_volume_1'],
                        data['ask_price_2'], data['ask_volume_2'],
                        data['ask_price_3'], data['ask_volume_3'],
                        data['ask_price_4'], data['ask_volume_4'],
                        data['ask_price_5'], data['ask_volume_5'],
                        data['extend_bid'], data['extend_bid_qty'],
                        data['extend_ask'], data['extend_ask_qty']
                    ))

                # ä½¿ç”¨ execute_values é€²è¡Œé«˜æ•ˆæ’å…¥
                execute_values(cursor, insert_sql, values_list, page_size=1000)
                inserted_count = len(batch_data)

                total_time = time.time() - start_time
                if len(batch_data) > 100:
                    logger.info(f"âœ… äº”æª” execute_values æ’å…¥å®Œæˆ: {inserted_count} ç­† (è€—æ™‚: {total_time:.2f}ç§’)")

                return inserted_count

            except ImportError:
                logger.warning("âš ï¸ execute_values ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨™æº– executemany")

                # æ¨™æº–æ‰¹é‡æ’å…¥æ–¹æ³•
                insert_sql = """
                    INSERT INTO best5_prices (
                        trade_datetime, symbol,
                        bid_price_1, bid_volume_1, bid_price_2, bid_volume_2,
                        bid_price_3, bid_volume_3, bid_price_4, bid_volume_4,
                        bid_price_5, bid_volume_5,
                        ask_price_1, ask_volume_1, ask_price_2, ask_volume_2,
                        ask_price_3, ask_volume_3, ask_price_4, ask_volume_4,
                        ask_price_5, ask_volume_5,
                        extend_bid, extend_bid_qty, extend_ask, extend_ask_qty
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (trade_datetime, symbol) DO NOTHING
                """

                values_list = []
                for data in batch_data:
                    values_list.append((
                        data['trade_datetime'],
                        data['symbol'],
                        data['bid_price_1'], data['bid_volume_1'],
                        data['bid_price_2'], data['bid_volume_2'],
                        data['bid_price_3'], data['bid_volume_3'],
                        data['bid_price_4'], data['bid_volume_4'],
                        data['bid_price_5'], data['bid_volume_5'],
                        data['ask_price_1'], data['ask_volume_1'],
                        data['ask_price_2'], data['ask_volume_2'],
                        data['ask_price_3'], data['ask_volume_3'],
                        data['ask_price_4'], data['ask_volume_4'],
                        data['ask_price_5'], data['ask_volume_5'],
                        data['extend_bid'], data['extend_bid_qty'],
                        data['extend_ask'], data['extend_ask_qty']
                    ))

                cursor.executemany(insert_sql, values_list)
                inserted_count = cursor.rowcount

                total_time = time.time() - start_time
                logger.info(f"âœ… äº”æª”æ¨™æº–æ’å…¥å®Œæˆ: {inserted_count} ç­† (è€—æ™‚: {total_time:.2f}ç§’)")

                return inserted_count

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥äº”æª”è³‡æ–™å¤±æ•—: {e}")
            return 0

    def import_best5_to_postgres(self, symbol='MTX00', batch_size=5000, optimize_performance=True):
        """åŒ¯å…¥äº”æª”è³‡æ–™åˆ°PostgreSQL"""
        # å°å…¥æ“´å±•åŠŸèƒ½
        from .postgres_importer_extensions import PostgreSQLImporterExtensions

        # å‰µå»ºä¸€å€‹æ··åˆé¡åˆ¥ä¾†ä½¿ç”¨æ“´å±•åŠŸèƒ½
        class MixedImporter(PostgreSQLImporterExtensions):
            def __init__(self, parent):
                self.postgres_initialized = parent.postgres_initialized
                self.sqlite_db_path = parent.sqlite_db_path
                self.convert_best5_to_postgres_format = parent.convert_best5_to_postgres_format
                self._insert_best5_batch_to_postgres = parent._insert_best5_batch_to_postgres

        mixed_importer = MixedImporter(self)
        return mixed_importer.import_best5_to_postgres(symbol, batch_size, optimize_performance)

    def import_all_data_to_postgres(self, symbol='MTX00', batch_size=5000, optimize_performance=True):
        """åŒ¯å…¥æ‰€æœ‰é¡å‹è³‡æ–™åˆ°PostgreSQL"""
        # å°å…¥æ“´å±•åŠŸèƒ½
        from .postgres_importer_extensions import PostgreSQLImporterExtensions

        # å‰µå»ºä¸€å€‹æ··åˆé¡åˆ¥ä¾†ä½¿ç”¨æ“´å±•åŠŸèƒ½
        class MixedImporter(PostgreSQLImporterExtensions):
            def __init__(self, parent):
                self.postgres_initialized = parent.postgres_initialized
                self.sqlite_db_path = parent.sqlite_db_path
                self.import_kline_to_postgres = parent.import_kline_to_postgres
                self.import_tick_to_postgres = parent.import_tick_to_postgres
                self.import_best5_to_postgres = parent.import_best5_to_postgres

        mixed_importer = MixedImporter(self)
        return mixed_importer.import_all_data_to_postgres(symbol, batch_size, optimize_performance)

    def get_postgres_data_statistics(self):
        """å–å¾—PostgreSQLè³‡æ–™çµ±è¨ˆ"""
        # å°å…¥æ“´å±•åŠŸèƒ½
        from .postgres_importer_extensions import PostgreSQLImporterExtensions

        # å‰µå»ºä¸€å€‹æ··åˆé¡åˆ¥ä¾†ä½¿ç”¨æ“´å±•åŠŸèƒ½
        class MixedImporter(PostgreSQLImporterExtensions):
            def __init__(self, parent):
                self.postgres_initialized = parent.postgres_initialized

        mixed_importer = MixedImporter(self)
        return mixed_importer.get_postgres_data_statistics()
